{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285eea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9fb220-4bc6-4731-83cd-049ff39f923f",
   "metadata": {},
   "source": [
    "# Environment Setup for Exploring ZeitGhost Backend\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/referencearchitectures/setup.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/referencearchitectures/setup.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/referencearchitectures/setup.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce7f45d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/zghost_overview.png\" width=\"1200\"/>\n",
    "</center>\n",
    "In order to build out the conversational agent which can use GDELT data and BigQuery public trends data to answer natural language questions, we need to first investigate what a suitable ACTOR_NAME for extracting the GDELT data could be, along with creating necessary cloud resources needed to build the architecture. \n",
    "\n",
    "Additionally, this notebook will save a persistent configuration file that you can reuse throughout the rest of the notebooks so the variables will be parameterized as you continue to work through the notebooks. \n",
    "\n",
    "### Get started with language models\n",
    "To view more information about getting started with language models on Vertex AI, see [Getting Started with the Vertex AI PaLM API & Python SDK](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb)\n",
    "\n",
    "### Environment Setup\n",
    "This notebook can be used to create configurations once that can be used for the rest of the notebooks to create the ZeitGhost Backend:\n",
    "1. [Setup Vertex Vector Store](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb)\n",
    "2. [GDELT DataOps](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb)\n",
    "3. [Vector Store Index Loader](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb) and 3a [Optional - Chunk up the Docs](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb)\n",
    "4. [Build Zeitghost Image](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb)\n",
    "5. [GDELT Pipelines](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb)\n",
    "6. [Plan and Execute Agents](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb)\n",
    "\n",
    "For more information, check out the [documentation on generative AI support for Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8e989",
   "metadata": {},
   "source": [
    "### Costs\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI Generative AI Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2efcae5",
   "metadata": {},
   "source": [
    "### Data security\n",
    "**Q: Does Google use customer data to improve its foundation models?**  \n",
    "A: No, Google does not use customer data to improve foundation models. Customer data is only used to generate a response from the model.\n",
    "\n",
    "**Q: Do Google employees see data that I submit to the model?**  \n",
    "A: No, Google employees have no access to customer data and all data is encrypted in-transit, in-use, and at-rest. \n",
    "\n",
    "**Q: Does Google store any of the customer data that is sent to the model?**  \n",
    "A: No, Google does not store customer data. However, Google may temporarily cache customer data for the duration of the request, such as prompt tuning pipeline and batch prediction. \n",
    "\n",
    "**Q: Does Google log data?**  \n",
    "A: No, Google does not log customer data. System-level logs help Google ensure system health and availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5934c2",
   "metadata": {},
   "source": [
    "### Responsible AI\n",
    "Large language models (LLMs) can translate language, summarize text, generate creative writing, generate code, power chatbots and virtual assistants, and complement search engines and recommendation systems. At the same time, as an early-stage technology, its evolving capabilities and uses create potential for misapplication, misuse, and unintended or unforeseen consequences. Large language models can generate output that you don't expect, including text that's offensive, insensitive, or factually incorrect.\n",
    "\n",
    "What's more, the incredible versatility of LLMs is also what makes it difficult to predict exactly what kinds of unintended or unforeseen outputs they might produce. Given these risks and complexities, the PaLM API is designed with [Google's AI Principles](https://ai.google/principles/) in mind. However, it is important for developers to understand and test their models to deploy safely and responsibly. To aid developers, the Generative AI Studio has built-in content filtering, and the PaLM API has safety attribute scoring to help customers test Google's safety filters and define confidence thresholds that are right for their use case and business. Please refer to the [Safety filters and attributes](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_filters_and_attributes) section to learn more.\n",
    "\n",
    "When the PaLM API is integrated into a customer's unique use case and context, additional responsible AI considerations and [PaLM limitations](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#palm_limitations) may need to be considered. We encourage customers to leverage fairness, interpretability, privacy and security [recommended practices](https://ai.google/responsibilities/responsible-ai-practices/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800112da",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10117931-4b39-45d3-b7a0-3836596aac2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4751ef-80de-4b4f-a062-8805fe1e648e",
   "metadata": {},
   "source": [
    "Run `pip` requirements.txt in either (1) the notebook cell below or (2) in a notebook terminal window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a8940c8-9b92-4caa-83f7-2956938ad8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "!pip install --no-cache-dir -r ./requirements.txt --user -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b3b2f3",
   "metadata": {},
   "source": [
    "### **IMPORTANT** Restart the Kernel\n",
    "\n",
    "**Colab only:** Uncomment the following cell to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe32256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf783b",
   "metadata": {},
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19240f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf0834-3a86-4ff6-8d99-7db84a7d391a",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e433a9d0-7686-4660-b0d2-4dfd62a2cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabba338-5e51-40c3-b30d-02d410d6b630",
   "metadata": {},
   "source": [
    "## Set Environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400de799-eaef-4d01-b5c4-5d5239bf3f62",
   "metadata": {},
   "source": [
    "* `PROJECT_NUM`         - manually enter (TODO: investigating single GCP `PROJECT_ID` to have many `PROJECT_NUM`s?)\n",
    "* `LOCATION` & `REGION` - location for GCS and Vertex AI assets\n",
    "* `BQ_LOCATION`         - location for BigQuery tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef8e5e8b-71ea-4217-ae17-a8a6659b28b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID       : wortz-project-352116\n",
      "PROJECT_NUM      : 679926387543\n",
      "LOCATION         : us-central1\n",
      "REGION           : us-central1\n",
      "BQ_LOCATION      : US\n",
      "VPC_NETWORK_NAME : me-network\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "PROJECT_NUM              = !gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\n",
    "PROJECT_NUM              = PROJECT_NUM[0]\n",
    "\n",
    "# locations / regions for cloud resources\n",
    "LOCATION                 = 'us-central1'        # TODO\n",
    "REGION                   = LOCATION             # TODO\n",
    "BQ_LOCATION              = 'US'                 # TODO\n",
    "\n",
    "# VPC network (TODO: public endpoints)\n",
    "VPC_NETWORK_NAME         = \"me-network\" # TODO\n",
    "\n",
    "print(f\"PROJECT_ID       : {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM      : {PROJECT_NUM}\")\n",
    "print(f\"LOCATION         : {LOCATION}\")\n",
    "print(f\"REGION           : {REGION}\")\n",
    "print(f\"BQ_LOCATION      : {BQ_LOCATION}\")\n",
    "print(f\"VPC_NETWORK_NAME : {VPC_NETWORK_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fb1a2-f6f6-46b5-9afa-41655c82c3d8",
   "metadata": {},
   "source": [
    "### Initialize Google Cloud SDK Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38d29ca-f70e-4c13-885a-cfcd92eca6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "vertex_ai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# bigquery client\n",
    "bqclient = bigquery.Client(\n",
    "    project=PROJECT_ID,\n",
    "    # location=LOCATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f180b-1304-4464-ad3a-9dea85ff694d",
   "metadata": {},
   "source": [
    "### Create new assets or use existing\n",
    "\n",
    "**`CREATE_NEW_ASSETS`**\n",
    "* `True` creates new GCS buckets, BQ tables, Matching Engine `Indexes` and `IndexEndpoints`\n",
    "* `False` uses existing \n",
    "\n",
    "**asset display names** use combination of the following variables:\n",
    "* `ACTOR_NAME`     - string, all lower case (e.g., \"google cloud\") representing the organization or company of interest. Used in BigQuery Public Dataset queries. See the [GDELT Event Cookbook](http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf) for detailed documentation.\n",
    "* `ACTOR_PREFIX`   - short customer prefix (e.g., `gcp`), for naming artifacts and GCP resources\n",
    "* `ACTOR_CATEGORY` - industry or category for the actor. used in LLM agent Q/A (e.g.,\"What will the impact of inflation have on the `{ACTOR_CATEGORY}` category over the next 6 months?\")\n",
    "* `VERSION`        - for development purposes and standardizing naming convention; use `VERSION` throughout notebooks to create multiple versions of assets, without having to create all new assets each time you want to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeeed990-79af-4937-9f34-7c7586457fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE_NEW_ASSETS : True\n",
      "ACTOR_PREFIX      : ggl\n",
      "VERSION           : v1\n",
      "ACTOR_NAME        : google\n",
      "ACTOR_CATEGORY    : technology\n"
     ]
    }
   ],
   "source": [
    "# create new BQ datasets, tables, etc.?\n",
    "CREATE_NEW_ASSETS         = True            # TODO: True | False\n",
    "\n",
    "# index IDs\n",
    "ACTOR_NAME                = 'google'         # TODO\n",
    "ACTOR_PREFIX              = 'ggl'            # TODO\n",
    "ACTOR_CATEGORY            = 'technology' # TODO\n",
    "VERSION                   = 'v1'             # TODO\n",
    "\n",
    "print(f\"CREATE_NEW_ASSETS : {CREATE_NEW_ASSETS}\")\n",
    "print(f\"ACTOR_PREFIX      : {ACTOR_PREFIX}\")\n",
    "print(f\"VERSION           : {VERSION}\")\n",
    "print(f\"ACTOR_NAME        : {ACTOR_NAME}\")\n",
    "print(f\"ACTOR_CATEGORY    : {ACTOR_CATEGORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959e9d8-e098-4641-8ad3-917b5d4ee374",
   "metadata": {},
   "source": [
    "## Test `ACTOR_NAME` in GDELT\n",
    "\n",
    "Before continuing to create the naming convention we will use throughout these notebooks, confirm your actor (`ACTOR_NAME`) can be found in both GDELT tables below.\n",
    "\n",
    "The GDELT queries look for articles between two dates specified by the user: (`min_date`, `max_date`)\n",
    "* Start small to avoid creating a massive dataframe in-notebook. To scale to larger time periods, you can use the pipeline functionality to orchestrate larger data extractions.\n",
    "* Increase the time window as needed\n",
    "* Ideally you can find 100s - 1000s of articles within the last few years. Some actors will meet this goal within a time window of a few days - great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f753d7-fa1e-430f-aa94-74afc7cd0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from zeitghost.gdelt.GdeltData import GdeltData\n",
    "from zeitghost.bigquery.BigQueryAccessor import BigQueryAccessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd5a70b-ae03-4554-bb35-f88b2761636a",
   "metadata": {},
   "source": [
    "### Test GDELT events table\n",
    "Set the date range you want to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66e14d3-fc37-4e97-90b1-d73d7b6fd323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN_DATE     : 2023-06-14\n",
      "MAX_DATE     : 2023-06-15\n"
     ]
    }
   ],
   "source": [
    "# set dates - '%Y%m%d'\n",
    "MIN_DATE = \"2023-06-14\" # TODO\n",
    "MAX_DATE = \"2023-06-15\" # TODO\n",
    "\n",
    "print(f\"MIN_DATE     : {MIN_DATE}\")\n",
    "print(f\"MAX_DATE     : {MAX_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe31317",
   "metadata": {},
   "source": [
    "Pass the actor name and date range to see which kinds of data for your selection is available from the GDELT events table, and preview it in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382d0424-a8cf-45cb-b93a-6f8ee490c820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SQLDATE</th>\n",
       "      <th>new_date</th>\n",
       "      <th>Actor1Name</th>\n",
       "      <th>Actor2Name</th>\n",
       "      <th>GoldsteinScale</th>\n",
       "      <th>NumMentions</th>\n",
       "      <th>NumSources</th>\n",
       "      <th>NumArticles</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>SOURCEURL</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230615</td>\n",
       "      <td>2023-06-15</td>\n",
       "      <td>None</td>\n",
       "      <td>GOOGLE</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.29249</td>\n",
       "      <td>https://news.yahoo.com/eu-files-antitrust-char...</td>\n",
       "      <td>https://news.yahoo.com/eu-files-antitrust-char...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SQLDATE    new_date Actor1Name Actor2Name  GoldsteinScale  NumMentions  \\\n",
       "0  20230615  2023-06-15       None     GOOGLE            -5.0           10   \n",
       "\n",
       "   NumSources  NumArticles  AvgTone  \\\n",
       "0           1           10 -2.29249   \n",
       "\n",
       "                                           SOURCEURL  \\\n",
       "0  https://news.yahoo.com/eu-files-antitrust-char...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://news.yahoo.com/eu-files-antitrust-char...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_data_accessor = BigQueryAccessor(\n",
    "    PROJECT_ID\n",
    "    , gdelt_project_id = 'gdelt-bq'\n",
    "    , gdelt_dataset_id = 'gdeltv2'\n",
    "    , gdelt_table_name = 'events'\n",
    ")\n",
    "\n",
    "# extract dataframe\n",
    "gdelt_events_accessor = events_data_accessor.get_records_from_actor_keyword_df(\n",
    "    keyword = ACTOR_NAME\n",
    "    , min_date = MIN_DATE\n",
    "    , max_date = MAX_DATE\n",
    ")\n",
    "\n",
    "print(gdelt_events_accessor.shape)\n",
    "gdelt_events_accessor.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a13ba22-ab51-41af-9ce7-921a0a73aef9",
   "metadata": {},
   "source": [
    "### Test GDELT entity table\n",
    "Set the date range you want to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbc9fda-db03-4ff0-b534-eccbd112f3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN_DATE     : 2023-01-14\n",
      "MAX_DATE     : 2023-06-15\n"
     ]
    }
   ],
   "source": [
    "# set dates - '%Y%m%d'\n",
    "MIN_DATE = \"2023-01-14\" # TODO\n",
    "MAX_DATE = \"2023-06-15\" # TODO\n",
    "\n",
    "print(f\"MIN_DATE     : {MIN_DATE}\")\n",
    "print(f\"MAX_DATE     : {MAX_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac7d2d",
   "metadata": {},
   "source": [
    "Pass the actor name and date range to see which kinds of data for your selection is available from the GDELT global entity table, and preview it in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "849b83be-1c22-4c03-a035-e564d003ebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29699, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>avgSalience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://gaceta.es/espana/sanchez-cambia-el-tab...</td>\n",
       "      <td>2023-05-30 05:46:42+00:00</td>\n",
       "      <td>0.167318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://gaceta.es/espana/sanchez-cambia-el-tab...   \n",
       "\n",
       "                       date  avgSalience  \n",
       "0 2023-05-30 05:46:42+00:00     0.167318  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geg_data_accessor = BigQueryAccessor(\n",
    "    PROJECT_ID\n",
    "    , gdelt_project_id='gdelt-bq'\n",
    "    , gdelt_dataset_id='gdeltv2'\n",
    "    , gdelt_table_name='geg_gcnlapi'\n",
    ")\n",
    "\n",
    "geg_articles_accessor = geg_data_accessor.get_geg_article_data_v2_full_df(\n",
    "    entity = ACTOR_NAME\n",
    "    , min_date = MIN_DATE\n",
    "    , max_date = MAX_DATE\n",
    ")\n",
    "\n",
    "print(geg_articles_accessor.shape)\n",
    "geg_articles_accessor.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42c678-652c-4e4c-b5c4-d2b80a2e4122",
   "metadata": {},
   "source": [
    "## Define Cloud Resource Names and Args\n",
    "\n",
    "After confirming your `ACTOR_NAME` can be found in both GDELT tables, proceed to set the resource names and args below.\n",
    "\n",
    "> Note: No need to edit the cell below. These values will be saved to a config file in GCS and loaded at the beginning of each subsequent notebook. This is intended to help keep track of the many assets used throughout these notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "710cc428-0525-4ab5-b610-a25113301da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_NAME               : zghost-ggl-v1-wortz-project-352116\n",
      "BUCKET_URI                : gs://zghost-ggl-v1-wortz-project-352116\n",
      "EMBEDDING_DIR_BUCKET_URI  : gs://zghost-ggl-v1-wortz-project-352116-emd-dir\n",
      "\n",
      "VPC_NETWORK_FULL          : projects/679926387543/global/networks/me-network\n",
      "ME_INDEX_NAME             : vectorstore_ggl_v1\n",
      "ME_DIMENSIONS             : 768\n",
      "MY_BQ_DATASET             : zghost_ggl_v1\n",
      "MY_BQ_TRENDS_DATASET      : zghost_ggl_v1_trends\n"
     ]
    }
   ],
   "source": [
    "# staging google cloud storage bucket\n",
    "BUCKET_NAME              = f'zghost-{ACTOR_PREFIX}-{VERSION}-{PROJECT_ID}'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "# bucket to stash embedding files\n",
    "EMBEDDING_DIR_BUCKET     = f'{BUCKET_NAME}-emd-dir'\n",
    "EMBEDDING_DIR_BUCKET_URI = f'gs://{EMBEDDING_DIR_BUCKET}'\n",
    "\n",
    "# vpc network\n",
    "VPC_NETWORK_FULL         = f\"projects/{PROJECT_NUM}/global/networks/{VPC_NETWORK_NAME}\"\n",
    "\n",
    "# matching engine vector store\n",
    "ME_INDEX_NAME            = f\"vectorstore_{ACTOR_PREFIX}_{VERSION}\"\n",
    "ME_DIMENSIONS            = 768 # when using Vertex PaLM Embedding\n",
    "\n",
    "# bigquery dataset\n",
    "MY_BQ_DATASET            = BUCKET_NAME.lower().replace(PROJECT_ID,\"\").replace(\"-\",\"_\").rstrip(\"_\")\n",
    "MY_BQ_TRENDS_DATASET     = f\"{MY_BQ_DATASET}_trends\"\n",
    "\n",
    "print(f\"BUCKET_NAME               : {BUCKET_NAME}\")\n",
    "print(f\"BUCKET_URI                : {BUCKET_URI}\")\n",
    "print(f\"EMBEDDING_DIR_BUCKET_URI  : {EMBEDDING_DIR_BUCKET_URI}\\n\")\n",
    "print(f\"VPC_NETWORK_FULL          : {VPC_NETWORK_FULL}\")\n",
    "print(f\"ME_INDEX_NAME             : {ME_INDEX_NAME}\")\n",
    "print(f\"ME_DIMENSIONS             : {ME_DIMENSIONS}\")\n",
    "print(f\"MY_BQ_DATASET             : {MY_BQ_DATASET}\")\n",
    "print(f\"MY_BQ_TRENDS_DATASET      : {MY_BQ_TRENDS_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd08cd-0165-45b2-b452-67531c2bde67",
   "metadata": {},
   "source": [
    "## Create Cloud Resources needed for these notebooks\n",
    "\n",
    "> Note: you will only need to do this once per `ACTOR_NAME` + `VERSION`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb4886-6d26-4fa4-82cc-4908794ef5f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create new GCS buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8090db2c-b332-4266-addd-0edf62b2c7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://zghost-way-v1-wortz-project-352116/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'zghost-way-v1-wortz-project-352116' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n",
      "Creating gs://zghost-way-v1-wortz-project-352116-emd-dir/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'zghost-way-v1-wortz-project-352116-emd-dir' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "if CREATE_NEW_ASSETS:\n",
    "    ! gsutil mb -l $LOCATION $BUCKET_URI\n",
    "    ! gsutil mb -l $LOCATION $EMBEDDING_DIR_BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d50641bd-7b93-4323-aace-c312c7721649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://embeddings_0.json [Content-Type=application/json]...\n",
      "/ [1 files][  3.8 KiB/  3.8 KiB]                                                \n",
      "Operation completed over 1 objects/3.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "if CREATE_NEW_ASSETS:\n",
    "    \n",
    "    # dummy embedding\n",
    "    init_embedding = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"embedding\": list(np.zeros(ME_DIMENSIONS))\n",
    "    }\n",
    "\n",
    "    # dump embedding to a local file\n",
    "    with open(\"embeddings_0.json\", \"w\") as f:\n",
    "        json.dump(init_embedding, f)\n",
    "\n",
    "    # write embedding to Cloud Storage\n",
    "    ! gsutil cp embeddings_0.json {EMBEDDING_DIR_BUCKET_URI}/init_index/embeddings_0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e395f7-23ea-4d04-8d67-340db61df3cd",
   "metadata": {},
   "source": [
    "### Create new BigQuery datasets\n",
    "\n",
    "For more information about working with the BigQuery Python SDK, see [SDK reference](https://cloud.google.com/python/docs/reference/bigquery/latest/index.html#google.cloud.bigquery.dataset.Dataset)\n",
    "\n",
    "Create two BQ datasets:\n",
    "* `MY_BQ_DATASET` will be used to store GDELT data of interest, including scraped article content\n",
    "* `MY_BQ_TRENDS_DATASET` will be used to copy the Google Trends public dataset, which will later be used when we combine the decision making ability of LLMs with tools in order to create a system that can execute and implement solutions (i.e., [agents](https://python.langchain.com/en/latest/use_cases/personal_assistants.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a7c37-7f93-42dd-9093-e78c03b3765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_NEW_ASSETS:\n",
    "    ds = bigquery.Dataset(f\"{PROJECT_ID}.{MY_BQ_DATASET}\")\n",
    "    ds.location = 'us' #Multi-region is REGION[0:2]\n",
    "    ds = bqclient.create_dataset(dataset = ds, exists_ok = False)\n",
    "\n",
    "    print(ds.full_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4008d9-3097-4a4c-b4dc-cfd50e1dd930",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_NEW_ASSETS:\n",
    "    ds = bigquery.Dataset(f\"{PROJECT_ID}.{MY_BQ_TRENDS_DATASET}\")\n",
    "    ds.location = 'us' #Multi-region is REGION[0:2]\n",
    "    ds = bqclient.create_dataset(dataset = ds, exists_ok = False)\n",
    "\n",
    "    print(ds.full_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc54eb-3f3c-42e9-9f3e-ce6d6c2a9649",
   "metadata": {},
   "source": [
    "### Save Notebook Configuration Data\n",
    "\n",
    "If you want to avoid having to re-enter these across notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e3a01d9-d2d4-4d1d-8e64-f4018003c606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"wortz-project-352116\"\n",
      "PROJECT_NUM              = \"679926387543\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"me-network\"\n",
      "\n",
      "CREATE_NEW_ASSETS        = \"True\"\n",
      "ACTOR_PREFIX             = \"ggl\"\n",
      "VERSION                  = \"v1\"\n",
      "ACTOR_NAME               = \"google\"\n",
      "ACTOR_CATEGORY           = \"technology\"\n",
      "\n",
      "BUCKET_NAME              = \"zghost-ggl-v1-wortz-project-352116\"\n",
      "EMBEDDING_DIR_BUCKET     = \"zghost-ggl-v1-wortz-project-352116-emd-dir\"\n",
      "\n",
      "BUCKET_URI               = \"gs://zghost-ggl-v1-wortz-project-352116\"\n",
      "EMBEDDING_DIR_BUCKET_URI = \"gs://zghost-ggl-v1-wortz-project-352116-emd-dir\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/679926387543/global/networks/me-network\"\n",
      "\n",
      "ME_INDEX_NAME            = \"vectorstore_ggl_v1\"\n",
      "ME_INDEX_ENDPOINT_NAME   = \"vectorstore_ggl_v1_endpoint\"\n",
      "ME_DIMENSIONS            = \"768\"\n",
      "\n",
      "MY_BQ_DATASET            = \"zghost_ggl_v1\"\n",
      "MY_BQ_TRENDS_DATASET     = \"zghost_ggl_v1_trends\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = f\"\"\"\n",
    "PROJECT_ID               = \\\"{PROJECT_ID}\\\"\n",
    "PROJECT_NUM              = \\\"{PROJECT_NUM}\\\"\n",
    "LOCATION                 = \\\"{LOCATION}\\\"\n",
    "\n",
    "REGION                   = \\\"{REGION}\\\"\n",
    "BQ_LOCATION              = \\\"{BQ_LOCATION}\\\"\n",
    "VPC_NETWORK_NAME         = \\\"{VPC_NETWORK_NAME}\\\"\n",
    "\n",
    "CREATE_NEW_ASSETS        = \\\"{CREATE_NEW_ASSETS}\\\"\n",
    "ACTOR_PREFIX             = \\\"{ACTOR_PREFIX}\\\"\n",
    "VERSION                  = \\\"{VERSION}\\\"\n",
    "ACTOR_NAME               = \\\"{ACTOR_NAME}\\\"\n",
    "ACTOR_CATEGORY           = \\\"{ACTOR_CATEGORY}\\\"\n",
    "\n",
    "BUCKET_NAME              = \\\"{BUCKET_NAME}\\\"\n",
    "EMBEDDING_DIR_BUCKET     = \\\"{EMBEDDING_DIR_BUCKET}\\\"\n",
    "\n",
    "BUCKET_URI               = \\\"{BUCKET_URI}\\\"\n",
    "EMBEDDING_DIR_BUCKET_URI = \\\"{EMBEDDING_DIR_BUCKET_URI}\\\"\n",
    "\n",
    "VPC_NETWORK_FULL         = \\\"{VPC_NETWORK_FULL}\\\"\n",
    "\n",
    "ME_INDEX_NAME            = \\\"{ME_INDEX_NAME}\\\"\n",
    "ME_INDEX_ENDPOINT_NAME   = \\\"{ME_INDEX_NAME}_endpoint\\\"\n",
    "ME_DIMENSIONS            = \\\"{ME_DIMENSIONS}\\\"\n",
    "\n",
    "MY_BQ_DATASET            = \\\"{MY_BQ_DATASET}\\\"\n",
    "MY_BQ_TRENDS_DATASET     = \\\"{MY_BQ_TRENDS_DATASET}\\\"\n",
    "\"\"\"\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f4a9af-393b-4ebe-99f8-f77263761d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying from <STDIN>...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "!echo '{config}' | gsutil cp - {BUCKET_URI}/config/notebook_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f94177",
   "metadata": {},
   "source": [
    "List out the bucket contents and verify access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d652adfa-9c3c-4a75-8c5f-0bafcd127141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://zghost-ggl-v1-wortz-project-352116/config/\n",
      "gs://zghost-ggl-v1-wortz-project-352116/pipeline_root/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300aca61-7aab-470e-b61d-09db21bbfe73",
   "metadata": {},
   "source": [
    "### Make a copy of the Google Trends Public Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3454c4b8-06e6-4ec3-b3e3-924651b2e101",
   "metadata": {},
   "source": [
    "In the `plan-and-execute` agents notebook, we will provide this public dataset as a tool in its information retrieval. Because we can't run an agent directly against a public dataset (permissions), let's make a copy of that dataset into our newly created BigQuery dataset\n",
    "\n",
    "![](imgs/public_trends_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ac6d2a3-dfa6-4b02-8728-ebaaca4bbe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_PUBLIC_DATA_PROJECT       = 'bigquery-public-data'\n",
    "TRENDS_DATASET               = 'google_trends'\n",
    "INTL_TOP_RISING_TRENDS_TABLE = 'international_top_rising_terms'\n",
    "INTL_TOP_TRENDS_TABLE        = 'international_top_terms'\n",
    "TOP_RISING_TRENDS            = 'top_rising_terms'\n",
    "TOP_TERMS_TABLE              = 'top_terms'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a78c8d",
   "metadata": {},
   "source": [
    "Create the international top rising trends table - you should see the print out containing information around how long the job took to run after it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56eca160-b7cc-47d9-a5aa-bf43041ba0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        CREATE OR REPLACE TABLE `wortz-project-352116.zghost_way_v1_trends.international_top_rising_terms` AS (\n",
      "            SELECT * FROM `bigquery-public-data.google_trends.international_top_rising_terms`\n",
      "        )\n",
      "    \n",
      "DONE 10.209\n"
     ]
    }
   ],
   "source": [
    "if CREATE_NEW_ASSETS:\n",
    "    # here\n",
    "    query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{PROJECT_ID}.{MY_BQ_TRENDS_DATASET}.{INTL_TOP_RISING_TRENDS_TABLE}` AS (\n",
    "            SELECT * FROM `{BQ_PUBLIC_DATA_PROJECT}.{TRENDS_DATASET}.{INTL_TOP_RISING_TRENDS_TABLE}`\n",
    "        )\n",
    "    \"\"\"\n",
    "    print(query)\n",
    "    job = bqclient.query(query)\n",
    "    job.result()\n",
    "    print(job.state, (job.ended-job.started).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07422be1",
   "metadata": {},
   "source": [
    "Create the international top trends table - you should see the print out containing information around how long the job took to run after it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44684c2f-14bb-4ba6-b028-6d0b05f9e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE 8.04\n"
     ]
    }
   ],
   "source": [
    "if CREATE_NEW_ASSETS:\n",
    "    query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{PROJECT_ID}.{MY_BQ_TRENDS_DATASET}.{INTL_TOP_TRENDS_TABLE}` AS (\n",
    "            SELECT * FROM `{BQ_PUBLIC_DATA_PROJECT}.{TRENDS_DATASET}.{INTL_TOP_TRENDS_TABLE}`\n",
    "        )\n",
    "    \"\"\"\n",
    "    # print(query)\n",
    "    job = bqclient.query(query)\n",
    "    job.result()\n",
    "    print(job.state, (job.ended-job.started).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd7cf8",
   "metadata": {},
   "source": [
    "Create the top rising trends table - you should see the print out containing information around how long the job took to run after it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c74ef80-7a4b-4751-9c07-e67fb0630b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE 5.97\n"
     ]
    }
   ],
   "source": [
    "if CREATE_NEW_ASSETS:\n",
    "    query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{PROJECT_ID}.{MY_BQ_TRENDS_DATASET}.{TOP_RISING_TRENDS}` AS (\n",
    "            SELECT * FROM `{BQ_PUBLIC_DATA_PROJECT}.{TRENDS_DATASET}.{TOP_RISING_TRENDS}`\n",
    "        )\n",
    "    \"\"\"\n",
    "    # print(query)\n",
    "    job = bqclient.query(query)\n",
    "    job.result()\n",
    "    print(job.state, (job.ended-job.started).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328ab37",
   "metadata": {},
   "source": [
    "Create the top terms trends table - you should see the print out containing information around how long the job took to run after it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c95f37-238a-4408-bbab-05ced9e76507",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_NEW_ASSETS:\n",
    "    query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{PROJECT_ID}.{MY_BQ_TRENDS_DATASET}.{TOP_TERMS_TABLE}` AS (\n",
    "            SELECT * FROM `{BQ_PUBLIC_DATA_PROJECT}.{TRENDS_DATASET}.{TOP_TERMS_TABLE}`\n",
    "        )\n",
    "    \"\"\"\n",
    "    # print(query)\n",
    "    job = bqclient.query(query)\n",
    "    job.result()\n",
    "    print(job.state, (job.ended-job.started).total_seconds())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

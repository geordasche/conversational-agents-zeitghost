{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a3c8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4ec90-df65-4910-b3bd-1aecf803b6a4",
   "metadata": {},
   "source": [
    "# Setting up Vector Stores with Vertex Matching Engine\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299bd214",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/zghost_overview_ME.png\" width=\"1200\"/>\n",
    "</center>\n",
    "When working with LLMs and conversational agents, how the data that they are accessing is stored is crucial - efficient data processing is more important than ever for applications involving large language models, genAI, and semantic search. Many of these new applications using large unstructured datasets use vector embeddings, a data representation containing semantic information that LLMs can use to answer questions and maintain in a long-term memory. \n",
    "\n",
    "In this application we will use a specialized database - a Vector Database - for handling embeddings, optimized for storage and querying capabilities for embeddings. The GDELT dataset extract could be quite large depending on the actor_name and time range, so we want to make sure that we aren't sacrificing performance to interact with such a potentially large dataset, which is where Vertex AI Matching Engine's Vector Database will ensure that we can scale for any very large number of embeddings.\n",
    "\n",
    "In this notebook you'll go through the process to create and deploy a vector store in Vertex Matching Engine. Whilst the setup may take 40-50min, once you've done this once, you can update, delete, and continue to add embeddings to this instance. \n",
    "\n",
    "---\n",
    "\n",
    "[Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview) provides the industry's leading high-scale low latency vector database. These vector databases are commonly referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.\n",
    "\n",
    "Matching Engine provides tooling to build use cases that match semantically similar items. More specifically, given a query item, Matching Engine finds the most semantically similar items to it from a large corpus of candidate items. This ability to search for semantically similar or semantically related items has many real world use cases and is a vital part of applications such as:\n",
    "\n",
    "* Recommendation engines\n",
    "* Search engines\n",
    "* Ad targeting systems\n",
    "* Image classification or image search\n",
    "* Text classification\n",
    "* Question answering\n",
    "* Chatbots\n",
    "\n",
    "To build semantic matching systems, you need to compute vector representations of all items. These vector representations are often called embeddings. Embeddings are computed by using machine learning models, which are trained to learn an embedding space where similar examples are close while dissimilar ones are far apart. The closer two items are in the embedding space, the more similar they are.\n",
    "\n",
    "At a high level, semantic matching can be simplified into two critical steps:\n",
    "\n",
    "* Generate embedding representations of items.\n",
    "* Perform nearest neighbor searches on embeddings.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "In this notebook, you will create a Vector Store using Vertex AI Matching Engine\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Installing the Python SDK \n",
    "- Create or initialize an existing matching engine index\n",
    "  - Creating a new index can take 40-50 minutes\n",
    "  - If you have already created an index and want to use this existing one, follow the instructions to initialize an existing index\n",
    "  - Whilst creating a new index, consider proceeding to [GDELT DataOps](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb) notebook\n",
    "- Create the Vector Store with embedddings, leveraging the embeddings model with `textembedding-gecko@001`\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dda8e7",
   "metadata": {},
   "source": [
    "### Costs\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI Generative AI Studio\n",
    "* Vertex AI Matching Engine\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c9095",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df5dc2",
   "metadata": {},
   "source": [
    "**Colab only:** Uncomment the following cell to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba34e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943f6fc",
   "metadata": {},
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d84780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4d6ec-0e4d-40da-8a03-e7038bab7485",
   "metadata": {},
   "source": [
    "### Make sure you edit the values below\n",
    "Each time you run the notebook for the first time with new variables, you just need to edit the actor prefix and version variables below. They are needed to grab all the other variables in the notebook configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b105ad1f-1b76-4551-a269-c31bc7b6da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTOR_PREFIX       : way\n",
      "VERSION            : v1\n"
     ]
    }
   ],
   "source": [
    "# CREATE_NEW_ASSETS        = True # True | False\n",
    "ACTOR_PREFIX             = \"way\"\n",
    "VERSION                  = 'v1'\n",
    "\n",
    "# print(f\"CREATE_NEW_ASSETS  : {CREATE_NEW_ASSETS}\")\n",
    "print(f\"ACTOR_PREFIX       : {ACTOR_PREFIX}\")\n",
    "print(f\"VERSION            : {VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f751b-e348-4357-a294-7bf4ba3a6ff5",
   "metadata": {},
   "source": [
    "### Load configuration settings from setup notebook\n",
    "Set the variables used in this notebook and load the config settings from the `00-env-setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef986cc-3211-4093-bce0-3bac431a07a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"wortz-project-352116\"\n",
      "PROJECT_NUM              = \"679926387543\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"me-network\"\n",
      "\n",
      "CREATE_NEW_ASSETS        = \"True\"\n",
      "ACTOR_PREFIX             = \"way\"\n",
      "VERSION                  = \"v1\"\n",
      "ACTOR_NAME               = \"wayfair\"\n",
      "ACTOR_CATEGORY           = \"retail\"\n",
      "\n",
      "BUCKET_NAME              = \"zghost-way-v1-wortz-project-352116\"\n",
      "EMBEDDING_DIR_BUCKET     = \"zghost-way-v1-wortz-project-352116-emd-dir\"\n",
      "\n",
      "BUCKET_URI               = \"gs://zghost-way-v1-wortz-project-352116\"\n",
      "EMBEDDING_DIR_BUCKET_URI = \"gs://zghost-way-v1-wortz-project-352116-emd-dir\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/679926387543/global/networks/me-network\"\n",
      "\n",
      "ME_INDEX_NAME            = \"vectorstore_way_v1\"\n",
      "ME_INDEX_ENDPOINT_NAME   = \"vectorstore_way_v1_endpoint\"\n",
      "ME_DIMENSIONS            = \"768\"\n",
      "\n",
      "MY_BQ_DATASET            = \"zghost_way_v1_wortz_project_352116\"\n",
      "MY_BQ_TRENDS_DATASET     = \"zghost_way_v1_wortz_project_352116_trends\"\n",
      "\n",
      "BUCKET_NAME        : zghost-way-v1-wortz-project-352116\n",
      "BUCKET_URI         : gs://zghost-way-v1-wortz-project-352116\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "BUCKET_NAME              = f'zghost-{ACTOR_PREFIX}-{VERSION}-{PROJECT_ID}'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)\n",
    "\n",
    "print(f\"BUCKET_NAME        : {BUCKET_NAME}\")\n",
    "print(f\"BUCKET_URI         : {BUCKET_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed27576-f85a-4b5b-a54a-e4f61b30dd4e",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98bbd868-e768-44a0-bf7c-862201209616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "# the following helper classes create and instantiate the matching engine resources\n",
    "from zeitghost.vertex.MatchingEngineCRUD import MatchingEngineCRUD\n",
    "from zeitghost.vertex.MatchingEngineVectorstore import MatchingEngineVectorStore\n",
    "from zeitghost.vertex.LLM import VertexLLM\n",
    "from zeitghost.vertex.Embeddings import VertexEmbeddings\n",
    "\n",
    "import uuid\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3944efc9-ee04-4b40-b4c4-64652beddf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID,location=LOCATION)\n",
    "\n",
    "# bigquery client\n",
    "bqclient = bigquery.Client(\n",
    "    project=PROJECT_ID,\n",
    "    # location=LOCATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97925b-22d6-457a-9e53-212de1ca3fdb",
   "metadata": {},
   "source": [
    "## Matching Engine Index: initialize existing or create a new one\n",
    "\n",
    "Validate access and bucket contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96dcdd09-9e35-419a-8347-2de086a6500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://zghost-way-v1-wortz-project-352116-emd-dir/init_index/embeddings_0.json\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls $EMBEDDING_DIR_BUCKET_URI/init_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d790f",
   "metadata": {},
   "source": [
    "Pass the required parameters that will be used to create the matching engine index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc7e096f-9784-4bbf-8512-bd3000db21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mengine = MatchingEngineCRUD(\n",
    "    project_id=PROJECT_ID \n",
    "    , project_num=PROJECT_NUM\n",
    "    , region=LOCATION \n",
    "    , index_name=ME_INDEX_NAME\n",
    "    , vpc_network_name=VPC_NETWORK_FULL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f7438-cf41-4c12-9785-f8a9a49bfde9",
   "metadata": {},
   "source": [
    "### Create or Initialize Existing Index\n",
    "\n",
    "Creating a Vertex Matching Engine index can take ~40-50 minutes due to the index compaction algorithm it uses to structure the index for high performance queries at scale. Read more about the [novel algorithm](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) proposed by Google Researchand the [official whitepaper](https://arxiv.org/abs/1908.10396)\n",
    "\n",
    "**Considering this setup time, proceed to Notebook `02-gdelt-data-ops.ipynb` to start extracting events and articles related to your actor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54160cb-3787-4608-9ede-ed2a6a89ee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Index vectorstore_way_v1 does not exists. Creating index ...\n",
      "INFO:root:Poll the operation to create index ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...."
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# create ME index\n",
    "me_index = mengine.create_index(\n",
    "    f\"{EMBEDDING_DIR_BUCKET_URI}/init_index\"\n",
    "    , int(ME_DIMENSIONS)\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"elapsed time: {end - start}\")\n",
    "\n",
    "if me_index:\n",
    "    print(me_index.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830a613-5510-4e7c-b821-50b90dbe1392",
   "metadata": {},
   "source": [
    "### Create or Initialize Index Endpoint\n",
    "Once your Matching Engine Index has been created, create an index endpoint where the Index will be deployed to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d12ee68-5ea0-435d-968c-8b09c4576eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "index_endpoint=mengine.create_index_endpoint(\n",
    "    endpoint_name=ME_INDEX_ENDPOINT_NAME\n",
    "    , network=VPC_NETWORK_FULL\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"elapsed time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea30d7",
   "metadata": {},
   "source": [
    "Print out the detailed information about the index endpoint and VPC network where it is deployed, and any indexes that are already deployed to that endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc517c3a-316f-46b4-ba07-e643a13c882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_endpoint:\n",
    "    print(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
    "    print(f\"Index endpoint VPC network name: {index_endpoint.network}\")\n",
    "    print(f\"Deployed indexes on the index endpoint:\")\n",
    "    for d in index_endpoint.deployed_indexes:\n",
    "        print(f\"    {d.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd8f2d-cb15-4007-952d-e8a496da7652",
   "metadata": {},
   "source": [
    "### Deploy Index to Index Endpoint\n",
    "To interact with a matching engine index, you'll need to deploy it to an endpoint, where you can customize the underlying infrastructure behind the endpoint. For example, you can specify the scaling properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe4a6f-833e-4632-9047-b770dd6521b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_NEW_ASSETS == 'True':\n",
    "    \n",
    "    index_endpoint = mengine.deploy_index(\n",
    "        index_name = ME_INDEX_NAME\n",
    "        , endpoint_name = ME_INDEX_ENDPOINT_NAME\n",
    "        , min_replica_count = 2\n",
    "        , max_replica_count = 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d860d",
   "metadata": {},
   "source": [
    "Print out the information about the matching engine resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0a04e-2d0f-4b15-b2c5-1551866697c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_endpoint:\n",
    "    print(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
    "    print(f\"Index endpoint VPC network name: {index_endpoint.network}\")\n",
    "    print(f\"Deployed indexes on the index endpoint:\")\n",
    "    for d in index_endpoint.deployed_indexes:\n",
    "        print(f\"    {d.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097dee06-6772-43a3-83c0-8ba3f94b7846",
   "metadata": {},
   "source": [
    "### Get Index and IndexEndpoint IDs\n",
    "Set the variable values and print out the resource details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffaf92a-bd91-482a-a51f-088429c1c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "ME_INDEX_RESOURCE_NAME, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()\n",
    "ME_INDEX_ID=ME_INDEX_RESOURCE_NAME.split(\"/\")[5]\n",
    "\n",
    "print(f\"ME_INDEX_RESOURCE_NAME  = {ME_INDEX_RESOURCE_NAME}\")\n",
    "print(f\"ME_INDEX_ENDPOINT_ID    = {ME_INDEX_ENDPOINT_ID}\")\n",
    "print(f\"ME_INDEX_ID             = {ME_INDEX_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785b0bd-a597-4284-b8e6-6ffb9d9bbe08",
   "metadata": {},
   "source": [
    "## Matching Engine Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6333c-2368-4ed0-8103-e431450d08b4",
   "metadata": {},
   "source": [
    "### Define Vertex LLM & Embeddings\n",
    "The base class to create the various LLMs can be found in in the root repository - in zeitghost.vertex the `LLM.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e0c0f-de69-4ffe-b039-6d233d4da80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VertexLLM(\n",
    "    stop=None \n",
    "    , temperature=0.0\n",
    "    , max_output_tokens=1000\n",
    "    , top_p=0.7\n",
    "    , top_k=40\n",
    ")\n",
    "\n",
    "# llm that can be used for a BigQuery agent, containing stopwords to prevent hallucinations and string parsing\n",
    "langchain_llm_for_bq = VertexLLM(\n",
    "    stop=['Observation:'] \n",
    "    , strip=True \n",
    "    , temperature=0.0\n",
    "    , max_output_tokens=1000\n",
    "    , top_p=0.7\n",
    "    , top_k=40\n",
    ")\n",
    "\n",
    "# llm that can be used for a pandas agent, containing stopwords to prevent hallucinations\n",
    "langchain_llm_for_pandas = VertexLLM(\n",
    "    stop=['Observation:']\n",
    "    , strip=False\n",
    "    , temperature=0.0\n",
    "    , max_output_tokens=1000\n",
    "    , top_p=0.7\n",
    "    , top_k=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029adebc",
   "metadata": {},
   "source": [
    "Let's ping the language model to ensure we are getting an expected response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b5ef5-4ffb-4594-807a-b6c4717a53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm('how are you doing today?')\n",
    "llm('In no more than 50 words, what can you tell me about the band Widespread Panic?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb1e91",
   "metadata": {},
   "source": [
    "Now let's call the VertexEmbeddings class which helps us get document embeddings using the [Vertex AI Embeddings model](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings). Make sure that your REQUESTS_PER_MINUTE does not exceed your project quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ece82-5e45-476c-a854-2d2aba646529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeitghost.vertex.Embeddings import VertexEmbeddings\n",
    "\n",
    "REQUESTS_PER_MINUTE = 299 # example project quota==300\n",
    "vertex_embedding = VertexEmbeddings(requests_per_minute=REQUESTS_PER_MINUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18fffa5-ad26-43e0-8af8-b023ff8aeae8",
   "metadata": {},
   "source": [
    "## Initialize Matching Engine Vector Store\n",
    "Finally, to interact with the matching engine instance initialize it with everything that you have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df037bcd-5bff-417c-988e-6ab4806acb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize vector store\n",
    "me = MatchingEngineVectorStore.from_components(\n",
    "    project_id=PROJECT_ID\n",
    "    # , project_num=PROJECT_NUM\n",
    "    , region=LOCATION\n",
    "    , gcs_bucket_name=EMBEDDING_DIR_BUCKET_URI\n",
    "    , embedding=vertex_embedding\n",
    "    , index_id=ME_INDEX_ID\n",
    "    , endpoint_id=ME_INDEX_ENDPOINT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfbda36",
   "metadata": {},
   "source": [
    "Validate that you have created the vector store with the Vertex embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f984ea1-040b-4e2d-b4e1-401721288228",
   "metadata": {},
   "outputs": [],
   "source": [
    "me.embedding"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcc5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876072e-dd8b-4163-976b-379a3e032138",
   "metadata": {},
   "source": [
    "# Orchestrate recurring GDELT Vector Store Data Updates with Vertex Pipelines\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9776866",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/zghost_overview_pipeline_steps.png\" width=\"1200\"/>\n",
    "</center>\n",
    "Given that the GDELT dataset is constantly updated with new information every 15 min, you may want to consider options for how to automate adding new data to your vector store on an regularly recurring basis. In this notebook we combine the previous notebook steps into a pipeline which will check to see if new data is available, if it is available it will generate the embeddings and upload the new vectors to the matching engine instance.\n",
    "\n",
    "Now that we have created the `Dockerfile_gdelt` image, this can be used to create the [Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline) (as the base image) to orchestrate the previous notebooks:\n",
    "1. [Setup Vertex Vector Store](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb)\n",
    "2. [GDELT DataOps](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb)\n",
    "3. [Vector Store Index Loader](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb)\n",
    "4. [Alternative document format embeddings](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/intro_palm_api.ipynb)\n",
    "\n",
    "Note that this pipeline will not recreate the resources each time it runs - if you have already created the Matching Engine Index, Index Endpoint, Vector Store, as long as the specification remains the same the pipeline will continue to update the existing resources. \n",
    "\n",
    "If you want to generate the resources for a NEW GDELT extraction + Vector Store DB, you'll want to update the input parameters accordingly. \n",
    "\n",
    "To get started with Vertex Pipelines, see [the Google Cloud Vertex AI Pipeline Samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/main/notebooks/official/pipelines).\n",
    "\n",
    "This type of pipeline could also be considered a data engineering pipeline in that it is largely focused around extracting updated data and inserting it into the Vector Store - depending on the skillsets and preferences of a team, it could also make sense to use [Cloud Composer](https://cloud.google.com/composer) which offers managed Apache Airflow for workflow orchestration of data engineering workflows. \n",
    "\n",
    "---\n",
    "\n",
    "### Objectives\n",
    "This notebook will:\n",
    "- Use the previously built custom container image (stored in the Artifact Registry) as the base image in pipeline step\n",
    "- Define Vertex Pipeline with the following components:\n",
    "    - Extracts all articles fitting specified criteria\n",
    "    - Calls the `GDELT` class to scrape the text and load it to a BigQuery table\n",
    "    - Create matching engine resources if they don't exist\n",
    "    - Load the vectors to the index\n",
    "- Set the pipeline arguments\n",
    "- Compile the pipeline\n",
    "- Submit the pipeline job\n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/pipeline-complete.png\" width=\"1000\"/>\n",
    "</center>\n",
    "\n",
    "After running this pipeline, you will be able to take advantage of Vertex AI Pipeline's automatic ML Metadata tracking - allowing you to track artifacts, changes, and metadata for each pipeline run over time. \n",
    "\n",
    "<center>\n",
    "<img src=\"imgs/pipeline_metadata.png\" width=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b68021",
   "metadata": {},
   "source": [
    "### Costs\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Cloud Build\n",
    "* Artifact Registry\n",
    "* Vertex AI Pipelines\n",
    "* BigQuery Storage & Compute\n",
    "* Vertex AI Matching Engine\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae9c4",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "**Colab only:** Uncomment the following cell to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4591206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4e296",
   "metadata": {},
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069c97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aec147c-a503-4150-be6a-2d540628f24e",
   "metadata": {},
   "source": [
    "### Make sure you edit the values below\n",
    "Each time you run the notebook for the first time with new variables, you just need to edit the actor prefix and version variables below. They are needed to grab all the other variables in the notebook configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfce248a-752c-495c-bf85-94ef15140c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTOR_PREFIX       : way\n",
      "VERSION            : v1\n"
     ]
    }
   ],
   "source": [
    "# CREATE_NEW_ASSETS        = True # True | False\n",
    "ACTOR_PREFIX             = \"way\"\n",
    "VERSION                  = 'v1'\n",
    "\n",
    "# print(f\"CREATE_NEW_ASSETS  : {CREATE_NEW_ASSETS}\")\n",
    "print(f\"ACTOR_PREFIX       : {ACTOR_PREFIX}\")\n",
    "print(f\"VERSION            : {VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef6a24-5506-49f6-83f1-79b468a23828",
   "metadata": {},
   "source": [
    "### Load configuration settings from setup notebook\n",
    "Set the constants used in this notebook and load the config settings from the `00-env-setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "001a41a4-486c-4db5-8760-8b4a8b798912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID               = \"wortz-project-352116\"\n",
      "PROJECT_NUM              = \"679926387543\"\n",
      "LOCATION                 = \"us-central1\"\n",
      "\n",
      "REGION                   = \"us-central1\"\n",
      "BQ_LOCATION              = \"US\"\n",
      "VPC_NETWORK_NAME         = \"me-network\"\n",
      "\n",
      "CREATE_NEW_ASSETS        = \"True\"\n",
      "ACTOR_PREFIX             = \"way\"\n",
      "VERSION                  = \"v1\"\n",
      "ACTOR_NAME               = \"wayfair\"\n",
      "ACTOR_CATEGORY           = \"retail\"\n",
      "\n",
      "BUCKET_NAME              = \"zghost-way-v1-wortz-project-352116\"\n",
      "EMBEDDING_DIR_BUCKET     = \"zghost-way-v1-wortz-project-352116-emd-dir\"\n",
      "\n",
      "BUCKET_URI               = \"gs://zghost-way-v1-wortz-project-352116\"\n",
      "EMBEDDING_DIR_BUCKET_URI = \"gs://zghost-way-v1-wortz-project-352116-emd-dir\"\n",
      "\n",
      "VPC_NETWORK_FULL         = \"projects/679926387543/global/networks/me-network\"\n",
      "\n",
      "ME_INDEX_NAME            = \"vectorstore_way_v1\"\n",
      "ME_INDEX_ENDPOINT_NAME   = \"vectorstore_way_v1_endpoint\"\n",
      "ME_DIMENSIONS            = \"768\"\n",
      "\n",
      "MY_BQ_DATASET            = \"zghost_way_v1\"\n",
      "MY_BQ_TRENDS_DATASET     = \"zghost_way_v1_trends\"\n",
      "\n",
      "BUCKET_NAME        : zghost-way-v1-wortz-project-352116\n",
      "BUCKET_URI         : gs://zghost-way-v1-wortz-project-352116\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "BUCKET_NAME              = f'zghost-{ACTOR_PREFIX}-{VERSION}-{PROJECT_ID}'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)\n",
    "\n",
    "print(f\"BUCKET_NAME        : {BUCKET_NAME}\")\n",
    "print(f\"BUCKET_URI         : {BUCKET_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a82b1-868d-4151-a20f-1fdc38d0192c",
   "metadata": {},
   "source": [
    "### Pipeline Base Image\n",
    "\n",
    "Here is the base image we will use for each pipeline component (step). This will a llow us to access the zeitghost codebase within the pipeline step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7f580a3-d79a-4a2c-8d14-ecdbfa5f58ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_BASE_IMAGE: us-central1-docker.pkg.dev/wortz-project-352116/zghost-way/gdelt-pipe-v1:latest\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_BASE_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/zghost-{ACTOR_PREFIX}/gdelt-pipe-{VERSION}:latest\"\n",
    "\n",
    "print(f\"PIPELINE_BASE_IMAGE: {PIPELINE_BASE_IMAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144251bf-93fc-4d58-9cdc-f18de5ef22dc",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a44869fd-39e3-440a-9ad2-41e25c25e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Pipelines\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "\n",
    "# Kubeflow SDK\n",
    "import kfp\n",
    "# from kfp.v2 import dsl\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact, Dataset, Input, \n",
    "    InputPath, Model, Output,\n",
    "    OutputPath, component\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2cb8243-7444-4930-ad64-8ad1881ec985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp version: 1.8.22\n",
      "vertex_ai SDK version: 1.25.0\n",
      "bigquery SDK version: 3.10.0\n"
     ]
    }
   ],
   "source": [
    "print(f'kfp version: {kfp.__version__}')\n",
    "print(f'vertex_ai SDK version: {vertex_ai.__version__}')\n",
    "print(f'bigquery SDK version: {bigquery.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9771519-f54a-4c45-a58b-87a716ac9c98",
   "metadata": {},
   "source": [
    "Instantiate Google cloud SDK clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0cfc3896-fff3-43d1-a085-069a4ca83e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Vertex client\n",
    "vertex_ai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# bigquery client\n",
    "bqclient = bigquery.Client(\n",
    "    project=PROJECT_ID,\n",
    "    # location=LOCATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7313c-1e37-4938-ad27-ae20e1513de1",
   "metadata": {},
   "source": [
    "## Create Pipeline Component Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66fe52b",
   "metadata": {},
   "source": [
    "Update root directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e3f2a46-c941-442d-8186-c2cc2fdd1f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = '..'\n",
    "os.chdir(root_path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58caecf-9fbd-4171-a6ab-10944f9f8b79",
   "metadata": {},
   "source": [
    "### Component for Setting Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "569a6862-1236-43bb-9e15-7c2fcb691c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/zghost-{ACTOR_PREFIX}/gdelt-pipe-{VERSION}:latest\"\n",
    ")\n",
    "def set_config(\n",
    "    project: str\n",
    "    , project_num: str\n",
    "    , location: str\n",
    "    , version: str\n",
    "    , actor_prefix: str\n",
    "    , actor_name: str\n",
    "    , vpc_network_name: str\n",
    "    , create_new_assets: str\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('bucket_name', str)\n",
    "    , ('bucket_uri', str)\n",
    "    , ('emb_bucket', str)\n",
    "    , ('emb_bucket_uri', str)\n",
    "    , ('vpc_network_full', str)\n",
    "    , ('me_index_name', str)\n",
    "    , ('me_index_endpoint_name', str)\n",
    "    , ('me_dimensions', int)\n",
    "    , ('my_bq_dataset', str)\n",
    "]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import uuid\n",
    "    import json\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    # ==========================\n",
    "    # set SDK clients\n",
    "    # ==========================\n",
    "    \n",
    "    # cloud storage client\n",
    "    storage_client = storage.Client(project=project)\n",
    "\n",
    "    # Vertex client\n",
    "    vertex_ai.init(project=project, location=location)\n",
    "\n",
    "    # bigquery client\n",
    "    bqclient = bigquery.Client(\n",
    "        project=project,\n",
    "        # location=LOCATION\n",
    "    )\n",
    "    \n",
    "    # ==========================\n",
    "    # set names and variables\n",
    "    # ==========================\n",
    "    \n",
    "    # staging GCS\n",
    "    BUCKET_NAME              = f'zghost-{actor_prefix}-{version}-{project}'\n",
    "    BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "    # bucket to stash emb files\n",
    "    EMBEDDING_DIR_BUCKET     = f'{BUCKET_NAME}-emd-dir'\n",
    "    EMBEDDING_DIR_BUCKET_URI = f'gs://{EMBEDDING_DIR_BUCKET}'\n",
    "\n",
    "    # vpc network\n",
    "    VPC_NETWORK_FULL         = f\"projects/{project_num}/global/networks/{vpc_network_name}\"\n",
    "\n",
    "    # matching engine vector store\n",
    "    ME_INDEX_NAME            = f\"vectorstore_{actor_prefix}_{version}\"\n",
    "    ME_DIMENSIONS            = 768 # when using Vertex PaLM Embedding\n",
    "    ME_INDEX_ENDPOINT_NAME   = f\"{ME_INDEX_NAME}_endpoint\"\n",
    "\n",
    "    # bigquery\n",
    "    MY_BQ_DATASET            = BUCKET_NAME.lower().replace(project,\"\").replace(\"-\",\"_\").rstrip(\"_\")\n",
    "    \n",
    "    logging.info(f\"BUCKET_NAME               : {BUCKET_NAME}\")\n",
    "    logging.info(f\"BUCKET_URI                : {BUCKET_URI}\")\n",
    "    logging.info(f\"EMBEDDING_DIR_BUCKET      : {EMBEDDING_DIR_BUCKET}\")\n",
    "    logging.info(f\"EMBEDDING_DIR_BUCKET_URI  : {EMBEDDING_DIR_BUCKET_URI}\\n\")\n",
    "    logging.info(f\"VPC_NETWORK_FULL          : {VPC_NETWORK_FULL}\")\n",
    "    logging.info(f\"ME_INDEX_NAME             : {ME_INDEX_NAME}\")\n",
    "    logging.info(f\"ME_DIMENSIONS             : {ME_DIMENSIONS}\")\n",
    "    logging.info(f\"ME_INDEX_ENDPOINT_NAME    : {ME_INDEX_ENDPOINT_NAME}\")\n",
    "    logging.info(f\"MY_BQ_DATASET             : {MY_BQ_DATASET}\")\n",
    "    \n",
    "    if create_new_assets == \"True\":\n",
    "    \n",
    "        # ==========================\n",
    "        # create GCS assets\n",
    "        # ==========================\n",
    "\n",
    "        # create staging GCS bucket\n",
    "        bucket = storage_client.bucket(BUCKET_NAME)\n",
    "        new_bucket = storage_client.create_bucket(bucket, location=location)\n",
    "        logging.info(f\"Created bucket {new_bucket.name} in {new_bucket.location}\")\n",
    "\n",
    "        # create embedding dir GCS bucket\n",
    "        bucket = storage_client.bucket(EMBEDDING_DIR_BUCKET)\n",
    "        new_bucket = storage_client.create_bucket(bucket, location=location)\n",
    "        logging.info(f\"Created bucket {new_bucket.name} in {new_bucket.location}\")\n",
    "\n",
    "        # ================================\n",
    "        # create initial embedding vector\n",
    "        # ================================\n",
    "        # dummy embedding\n",
    "        init_embedding = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"embedding\": list(np.zeros(ME_DIMENSIONS))\n",
    "        }\n",
    "\n",
    "        LOCAL_JSON_FILE = \"embeddings_0.json\"\n",
    "\n",
    "        # dump embedding to a local file\n",
    "        with open(LOCAL_JSON_FILE, \"w\") as f:\n",
    "            json.dump(init_embedding, f)\n",
    "\n",
    "        GCS_BLOB = f'init_index/{LOCAL_JSON_FILE}'\n",
    "\n",
    "        bucket_client = storage_client.bucket(EMBEDDING_DIR_BUCKET)\n",
    "        blob = bucket_client.blob(GCS_BLOB)\n",
    "        blob.upload_from_filename(LOCAL_JSON_FILE)\n",
    "        logging.info(f\"uploaded init embedding to gs://{EMBEDDING_DIR_BUCKET}/{GCS_BLOB}\")\n",
    "\n",
    "        # ================================\n",
    "        # Create BQ dataset\n",
    "        # ================================\n",
    "        ds = bigquery.Dataset(f\"{project}.{MY_BQ_DATASET}\")\n",
    "        ds.location = 'us' #Multi-region is REGION[0:2]\n",
    "        ds = bqclient.create_dataset(dataset = ds, exists_ok = False)\n",
    "\n",
    "        ds.full_dataset_id\n",
    "        logging.info(f\"created {ds.full_dataset_id}\")\n",
    "        \n",
    "    else:\n",
    "        logging.info(\"No new assets created...\")\n",
    "    \n",
    "    return (\n",
    "        BUCKET_NAME\n",
    "        , BUCKET_URI\n",
    "        , EMBEDDING_DIR_BUCKET\n",
    "        , EMBEDDING_DIR_BUCKET_URI\n",
    "        , VPC_NETWORK_FULL\n",
    "        , ME_INDEX_NAME\n",
    "        , ME_INDEX_ENDPOINT_NAME\n",
    "        , ME_DIMENSIONS\n",
    "        , MY_BQ_DATASET\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d62b7-d94b-4977-a079-0c768a0e3e88",
   "metadata": {},
   "source": [
    "### Component to Extract and Scrape GDELT Events Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5228c677-1170-48a3-bbfd-491bc8f9d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/zghost-{ACTOR_PREFIX}/gdelt-pipe-{VERSION}:latest\"\n",
    ")\n",
    "def create_gdelt_events_table(\n",
    "    project: str\n",
    "    , location: str\n",
    "    , version: str\n",
    "    , bq_dataset: str\n",
    "    , actor_prefix: str\n",
    "    , actor_name: str\n",
    "    , min_date: str\n",
    "    , max_date: str\n",
    "    , test_table_prefix: str\n",
    "    , bq_location: str = 'US'\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('raw_articles_table', str)\n",
    "    , ('scraped_articles_table', str),\n",
    "]):\n",
    "    import logging\n",
    "    # ====================================\n",
    "    # import packages\n",
    "    # ====================================\n",
    "    logging.info(f\"importing packages...\")\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery as bq\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    \n",
    "    from zeitghost.gdelt.GdeltData import GdeltData\n",
    "    from zeitghost.bigquery.BigQueryAccessor import BigQueryAccessor\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    # bigquery client\n",
    "    bqclient = bq.Client(\n",
    "        project=project,\n",
    "        # location=LOCATION\n",
    "    )\n",
    "    # ====================================\n",
    "    # setting vars\n",
    "    # ====================================\n",
    "    logging.info(f\"setting vars...\")\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    if test_table_prefix == 'True':\n",
    "        GDELT_TABLE_NAME = f'test_events_gdelt_{actor_prefix}_{version}'\n",
    "    else:\n",
    "        GDELT_TABLE_NAME = f'events_gdelt_{actor_prefix}_{version}'\n",
    "    \n",
    "    # # Define table name, in format dataset.table_name\n",
    "    GDELT_TABLE_REF = f'{project}.{bq_dataset}.{GDELT_TABLE_NAME}'\n",
    "    SCRAPED_GDELT_TABLE_REF = f'{project}.{bq_dataset}.scraped_{GDELT_TABLE_NAME}'\n",
    "    \n",
    "    logging.info(f\"actor_prefix: {actor_prefix}\")\n",
    "    logging.info(f\"GDELT_TABLE_NAME: {GDELT_TABLE_NAME}\")\n",
    "    logging.info(f\"GDELT_TABLE_REF: {GDELT_TABLE_REF}\")\n",
    "    logging.info(f\"SCRAPED_GDELT_TABLE_REF: {SCRAPED_GDELT_TABLE_REF}\")\n",
    "    \n",
    "    # ====================================\n",
    "    # GDELT BigQueryAccessor\n",
    "    # ====================================\n",
    "    logging.info(f\"creating GDELT BigQueryAccessor...\")\n",
    "    \n",
    "    geg_data_accessor = BigQueryAccessor(\n",
    "        project\n",
    "        , gdelt_project_id='gdelt-bq'\n",
    "        , gdelt_dataset_id='gdeltv2'\n",
    "        , gdelt_table_name='events' # geg_gcnlapi\n",
    "    )\n",
    "    \n",
    "    logging.info(\"getting articles df...\")\n",
    "    start = time.time()\n",
    "    gdelt_events_accessor = geg_data_accessor.get_records_from_actor_keyword_df(\n",
    "        actor_name\n",
    "        , min_date #= \"2023-05-01\"\n",
    "        , max_date #= \"2023-05-26\"\n",
    "    )\n",
    "    end = time.time()\n",
    "    logging.info(f\"elapsed time: {end - start}\")\n",
    "    logging.info(f\"gdelt_events_accessor.shape: {gdelt_events_accessor.shape}\")\n",
    "    \n",
    "    # ====================================\n",
    "    # Load data to BQ\n",
    "    # ====================================\n",
    "    logging.info(f\"loading GDELT event articles df to {GDELT_TABLE_REF}\")\n",
    "    job = bqclient.load_table_from_dataframe(\n",
    "        gdelt_events_accessor\n",
    "        , GDELT_TABLE_REF\n",
    "    )\n",
    "    job.result()  # Wait for the job to complete.\n",
    "    \n",
    "    # update table description\n",
    "    TABLE_DESCRIPTION = f\"articles publsihed between `{min_date}` to `{max_date}` mentioning {actor_name} from `gdelt-bq.gdeltv2.events\"\n",
    "    logging.info(f\"updating {GDELT_TABLE_REF} description to: {TABLE_DESCRIPTION}\")\n",
    "    \n",
    "    table = bqclient.get_table(GDELT_TABLE_REF)  # API request\n",
    "    table.description = f'{TABLE_DESCRIPTION}'\n",
    "    table = bqclient.update_table(table, [\"description\"])  # API request\n",
    "    logging.info(f\"{GDELT_TABLE_REF} description updated\")\n",
    "    \n",
    "    if gdelt_events_accessor.shape[0] > 0:\n",
    "        logging.info(f\"gdelt_events_accessor.shape[0] greater than zero\")\n",
    "    \n",
    "        # ====================================\n",
    "        # GDELT scraper\n",
    "        # ====================================\n",
    "        logging.info(f\"Scraping GDELT articles for: {SCRAPED_GDELT_TABLE_REF}\")\n",
    "        start = time.time()\n",
    "\n",
    "        gdelt_data_processor = GdeltData(\n",
    "            gdelt_events_accessor\n",
    "            , destination_table=SCRAPED_GDELT_TABLE_REF\n",
    "            , destination_dataset=bq_dataset\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        logging.info(f\"elapsed time: {end - start}\")\n",
    "\n",
    "        events_full_source_df = pd.DataFrame(gdelt_data_processor.full_source_data)\n",
    "        logging.info(f\"events_full_source_df.shape: {events_full_source_df.shape}\")\n",
    "    \n",
    "    # if events_full_source_df.shape[0] > 0:\n",
    "    #     logging.info(f\"events_full_source_df.shape[0] greater than zero\")\n",
    "    \n",
    "        events_full_source_df = events_full_source_df.loc[events_full_source_df['article_count'] != 0]\n",
    "        logging.info(f\"events_full_source_df.shape: {events_full_source_df.shape}\")\n",
    "\n",
    "        events_full_source_article_list = events_full_source_df['articles']\n",
    "        logging.info(f\"len(geg_articles_article_list): {len(events_full_source_article_list)}\")\n",
    "\n",
    "        # ====================================\n",
    "        # format df to load to BQ\n",
    "        # ====================================\n",
    "        logging.info(f\"format df to load to BQ...\")\n",
    "        article_rows = []\n",
    "\n",
    "        for entry in events_full_source_article_list:\n",
    "            for ent in entry:\n",
    "                article_rows.append(ent)\n",
    "\n",
    "        if len(article_rows) > 0:\n",
    "            # article_rows[0]\n",
    "            events_source_article_df = pd.DataFrame(article_rows)\n",
    "            events_source_article_df.drop(columns='authors', inplace=True) # TODO - fix\n",
    "            events_source_article_df['source']=events_source_article_df['url']\n",
    "\n",
    "            logging.info(f\"events_source_article_df.shape: {events_source_article_df.shape}\")\n",
    "\n",
    "            # ====================================\n",
    "            # Load data to BQ\n",
    "            # ====================================\n",
    "            logging.info(f\"loading scraped articles df to {SCRAPED_GDELT_TABLE_REF}\")\n",
    "            job = bqclient.load_table_from_dataframe(\n",
    "                events_source_article_df\n",
    "                , SCRAPED_GDELT_TABLE_REF\n",
    "            )\n",
    "            job.result()  # Wait for the job to complete.\n",
    "\n",
    "            logging.info(f\"scraped dataframe loaded to: {SCRAPED_GDELT_TABLE_REF}\")\n",
    "\n",
    "            # ====================================\n",
    "            # update table description\n",
    "            # ====================================\n",
    "            TABLE_DESCRIPTION = f\"scraped articles mentioning {actor_name} published between`{min_date}` to `{max_date}` from `gdelt-bq.gdeltv2.events\"\n",
    "            logging.info(f\"updating {SCRAPED_GDELT_TABLE_REF} description to: {TABLE_DESCRIPTION}\")\n",
    "\n",
    "            table = bqclient.get_table(SCRAPED_GDELT_TABLE_REF)  # API request\n",
    "            table.description = f'{TABLE_DESCRIPTION}'\n",
    "            table = bqclient.update_table(table, [\"description\"])  # API request\n",
    "            logging.info(f\"{SCRAPED_GDELT_TABLE_REF} description updated\")\n",
    "\n",
    "        else:\n",
    "            print(\"No new articles...\")\n",
    "            SCRAPED_GDELT_TABLE_REF = ''\n",
    "\n",
    "    else:\n",
    "        print(\"No new articles...\")\n",
    "        SCRAPED_GDELT_TABLE_REF = ''\n",
    "    \n",
    "    return(\n",
    "        GDELT_TABLE_REF\n",
    "        , SCRAPED_GDELT_TABLE_REF\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e82b730-82a7-41e9-8343-30570e91a35f",
   "metadata": {},
   "source": [
    "### Component to Extract and Scrape GDELT GEG (Entity) Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e4823ff-373b-4452-b434-1aac6d4cac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/zghost-{ACTOR_PREFIX}/gdelt-pipe-{VERSION}:latest\"\n",
    ")\n",
    "def create_geg_article_table(\n",
    "    project: str\n",
    "    , location: str\n",
    "    , version: str\n",
    "    , bq_dataset: str\n",
    "    , actor_prefix: str\n",
    "    , actor_name: str\n",
    "    , min_date: str\n",
    "    , max_date: str\n",
    "    , test_table_prefix: str\n",
    "    , bq_location: str = 'US'\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('raw_articles_table', str)\n",
    "    , ('scraped_articles_table', str)\n",
    "]):\n",
    "\n",
    "    import logging\n",
    "    # ====================================\n",
    "    # import packages\n",
    "    # ====================================\n",
    "    \n",
    "    logging.info(f\"importing packages...\")\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery as bq\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    \n",
    "    from zeitghost.gdelt.GdeltData import GdeltData\n",
    "    from zeitghost.bigquery.BigQueryAccessor import BigQueryAccessor\n",
    "    \n",
    "    # ======================================\n",
    "    # set SDK clients & variables\n",
    "    # ======================================\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    # bigquery client\n",
    "    bqclient = bq.Client(\n",
    "        project=project,\n",
    "        # location=LOCATION\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    if test_table_prefix == 'True':\n",
    "        GDELT_TABLE_NAME = f'test_geg_articles_{actor_prefix}_{version}'\n",
    "    else:\n",
    "        GDELT_TABLE_NAME = f'geg_articles_{actor_prefix}_{version}'\n",
    "    \n",
    "    # # Define table name, in format dataset.table_name\n",
    "    GDELT_TABLE_REF = f'{project}.{bq_dataset}.{GDELT_TABLE_NAME}'\n",
    "    SCRAPED_GDELT_TABLE_REF = f'{project}.{bq_dataset}.scraped_{GDELT_TABLE_NAME}'\n",
    "    \n",
    "    logging.info(f\"actor_prefix: {actor_prefix}\")\n",
    "    logging.info(f\"GDELT_TABLE_NAME: {GDELT_TABLE_NAME}\")\n",
    "    logging.info(f\"GDELT_TABLE_REF: {GDELT_TABLE_REF}\")\n",
    "    logging.info(f\"SCRAPED_GDELT_TABLE_REF: {SCRAPED_GDELT_TABLE_REF}\")\n",
    "    \n",
    "    # ======================================\n",
    "    # extract GDELT data\n",
    "    # ======================================\n",
    "    geg_data_accessor = BigQueryAccessor(\n",
    "        project\n",
    "        , gdelt_project_id='gdelt-bq'\n",
    "        , gdelt_dataset_id='gdeltv2'\n",
    "        , gdelt_table_name='geg_gcnlapi' # geg_gcnlapi\n",
    "    )\n",
    "    \n",
    "    logging.info(\"getting articles df...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    geg_articles_accessor = geg_data_accessor.get_geg_article_data_v2_full_df(\n",
    "        actor_name\n",
    "        , min_date\n",
    "        , max_date\n",
    "    )\n",
    "    end = time.time()\n",
    "    logging.info(f\"elapsed time: {end - start}\")\n",
    "    logging.info(f\"geg_articles_accessor.shape: {geg_articles_accessor.shape}\")\n",
    "    \n",
    "    # ======================================\n",
    "    # load GDELT to BQ table\n",
    "    # ======================================\n",
    "    logging.info(f\"loading geg articles df to {GDELT_TABLE_REF}\")\n",
    "    job = bqclient.load_table_from_dataframe(\n",
    "        geg_articles_accessor\n",
    "        , GDELT_TABLE_REF\n",
    "    )\n",
    "    job.result()  # Wait for the job to complete.\n",
    "    \n",
    "    # update table description\n",
    "    TABLE_DESCRIPTION = f\"articles publsihed between `{min_date}` to `{max_date}` mentioning `{actor_name}` from `gdelt-bq.gdeltv2.geg_gcnlapi`\"\n",
    "    logging.info(f\"updating {GDELT_TABLE_REF} description to: {TABLE_DESCRIPTION}\")\n",
    "    \n",
    "    table = bqclient.get_table(GDELT_TABLE_REF)  # API request\n",
    "    table.description = f'{TABLE_DESCRIPTION}'\n",
    "    table = bqclient.update_table(table, [\"description\"])  # API request\n",
    "    logging.info(f\"{GDELT_TABLE_REF} description updated\")\n",
    "    \n",
    "    if geg_articles_accessor.shape[0] > 0:\n",
    "        logging.info(f\"geg_articles_accessor.shape[0] greater than zero\")\n",
    "    \n",
    "        # ====================================\n",
    "        # GDELT scraper\n",
    "        # ====================================\n",
    "        logging.info(f\"Scraping GDELT articles for: {SCRAPED_GDELT_TABLE_REF}\")\n",
    "        start = time.time()\n",
    "\n",
    "        gdelt_data_processor = GdeltData(\n",
    "            geg_articles_accessor\n",
    "            , destination_table=SCRAPED_GDELT_TABLE_REF\n",
    "            , destination_dataset=bq_dataset\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        logging.info(f\"elapsed time: {end - start}\")\n",
    "\n",
    "        geg_articles_full_source_df = pd.DataFrame(gdelt_data_processor.full_source_data)\n",
    "        logging.info(f\"geg_articles_full_source_df.shape: {geg_articles_full_source_df.shape}\")\n",
    "    \n",
    "    # if geg_articles_full_source_df.shape[0] > 0:\n",
    "    #     logging.info(f\"geg_articles_full_source_df.shape[0] greater than zero\")\n",
    "    \n",
    "        geg_articles_full_source_df = geg_articles_full_source_df.loc[geg_articles_full_source_df['article_count'] != 0]\n",
    "        logging.info(f\"geg_articles_full_source_df.shape: {geg_articles_full_source_df.shape}\")\n",
    "\n",
    "        geg_articles_article_list = geg_articles_full_source_df['articles']\n",
    "        logging.info(f\"len(geg_articles_article_list): {len(geg_articles_article_list)}\")\n",
    "\n",
    "        # format df to load to BQ\n",
    "        article_rows = []\n",
    "\n",
    "        for entry in geg_articles_article_list:\n",
    "            for ent in entry:\n",
    "                article_rows.append(ent)\n",
    "\n",
    "        if len(article_rows) > 0:\n",
    "            geg_articles_df = pd.DataFrame(article_rows)\n",
    "            geg_articles_df.drop(columns=['authors','NumMentions'], inplace=True) # TODO - fix\n",
    "            geg_articles_df['source']=geg_articles_df['url']\n",
    "            logging.info(f\"geg_articles_df.shape: {geg_articles_df.shape}\")\n",
    "\n",
    "            # Load data to BQ\n",
    "            logging.info(f\"loading scraped articles df to {SCRAPED_GDELT_TABLE_REF}\")\n",
    "            job = bqclient.load_table_from_dataframe(\n",
    "                geg_articles_df\n",
    "                , SCRAPED_GDELT_TABLE_REF\n",
    "            )\n",
    "            job.result()  # Wait for the job to complete.\n",
    "\n",
    "            logging.info(f\"scraped dataframe loaded to: {SCRAPED_GDELT_TABLE_REF}\")\n",
    "\n",
    "            # update table description\n",
    "            TABLE_DESCRIPTION = f\"scraped articles from `{min_date}` to `{max_date}` mentioning {actor_name} from `gdelt-bq.gdeltv2.geg_gcnlapi`\"\n",
    "            logging.info(f\"updating {SCRAPED_GDELT_TABLE_REF} description to: {TABLE_DESCRIPTION}\")\n",
    "\n",
    "            table = bqclient.get_table(SCRAPED_GDELT_TABLE_REF)  # API request\n",
    "            table.description = f'{TABLE_DESCRIPTION}'\n",
    "            table = bqclient.update_table(table, [\"description\"])  # API request\n",
    "            logging.info(f\"{SCRAPED_GDELT_TABLE_REF} description updated\")\n",
    "\n",
    "        else:\n",
    "            print(\"No new articles...\")\n",
    "            SCRAPED_GDELT_TABLE_REF = ''\n",
    "            \n",
    "    else:\n",
    "        print(\"No new articles...\")\n",
    "        SCRAPED_GDELT_TABLE_REF = ''\n",
    "    \n",
    "    return(\n",
    "        GDELT_TABLE_REF\n",
    "        , SCRAPED_GDELT_TABLE_REF\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398fd927-19b7-4be0-ac76-588c63c0270f",
   "metadata": {},
   "source": [
    "### Component to Create Matching Engine Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4159a8bb-3deb-4165-992f-b3f5ab320802",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/zghost-{ACTOR_PREFIX}/gdelt-pipe-{VERSION}:latest\"\n",
    ")\n",
    "def create_me_vectorstore(\n",
    "    project: str\n",
    "    , project_num: str\n",
    "    , location: str\n",
    "    , version: str\n",
    "    , bq_dataset: str\n",
    "    , me_index_name: str\n",
    "    , me_index_endpoint_name: str\n",
    "    , vpc_network_full: str\n",
    "    , embedding_dir_bucket_uri: str\n",
    "    , me_dimensions: int\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('me_index_resource_name', str)\n",
    "    , ('me_index_endpoint_id', str)\n",
    "    , ('me_index_id', str)\n",
    "    , ('me_index', Artifact)\n",
    "    , ('index_endpoint', Artifact)\n",
    "]):\n",
    "    import logging\n",
    "    # ======================================\n",
    "    # Import packages\n",
    "    # ======================================\n",
    "    logging.info(f\"Importing packages...\")\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery as bq\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    \n",
    "    from zeitghost.vertex.MatchingEngineCRUD import MatchingEngineCRUD\n",
    "    from zeitghost.vertex.MatchingEngineVectorstore import MatchingEngineVectorStore\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    # bigquery client\n",
    "    bqclient = bq.Client(\n",
    "        project=project,\n",
    "        # location=LOCATION\n",
    "    )\n",
    "    \n",
    "    # ======================================\n",
    "    # create mengine\n",
    "    # ======================================\n",
    "    logging.info(f\"create mengine...\")\n",
    "    \n",
    "    mengine = MatchingEngineCRUD(\n",
    "        project_id=project \n",
    "        , project_num=project_num\n",
    "        , region=location \n",
    "        , index_name=me_index_name\n",
    "        , vpc_network_name=vpc_network_full\n",
    "    )\n",
    "    \n",
    "    # ======================================\n",
    "    # create or get ME index \n",
    "    # ======================================\n",
    "    logging.info(f\"create or get ME index...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    me_index = mengine.create_index(\n",
    "        f\"{embedding_dir_bucket_uri}/init_index\"\n",
    "        , me_dimensions\n",
    "    )\n",
    "    end = time.time()\n",
    "    logging.info(f\"elapsed time: {end - start}\")\n",
    "\n",
    "    if me_index:\n",
    "        logging.info(me_index.name)\n",
    "        \n",
    "    # ======================================\n",
    "    # create or get ME index endpoint\n",
    "    # ======================================\n",
    "    logging.info(f\"create or get ME index endpoint...\")\n",
    "        \n",
    "    start = time.time()\n",
    "    index_endpoint=mengine.create_index_endpoint(\n",
    "        endpoint_name=me_index_endpoint_name\n",
    "        , network=vpc_network_full\n",
    "    )\n",
    "    end = time.time()\n",
    "    logging.info(f\"elapsed time: {end - start}\")\n",
    "    \n",
    "    if index_endpoint:\n",
    "        logging.info(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
    "        logging.info(f\"Index endpoint VPC network name: {index_endpoint.network}\")\n",
    "        logging.info(f\"Deployed indexes on the index endpoint:\")\n",
    "        for d in index_endpoint.deployed_indexes:\n",
    "            logging.info(f\"    {d.id}\")\n",
    "            \n",
    "    # get index & index endpoint IDs\n",
    "    ME_INDEX_RESOURCE_NAME, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()\n",
    "    ME_INDEX_ID=ME_INDEX_RESOURCE_NAME.split(\"/\")[5]\n",
    "\n",
    "    logging.info(f\"ME_INDEX_RESOURCE_NAME  = {ME_INDEX_RESOURCE_NAME}\")\n",
    "    logging.info(f\"ME_INDEX_ENDPOINT_ID    = {ME_INDEX_ENDPOINT_ID}\")\n",
    "    logging.info(f\"ME_INDEX_ID             = {ME_INDEX_ID}\")\n",
    "    \n",
    "    return (\n",
    "        ME_INDEX_RESOURCE_NAME\n",
    "        , ME_INDEX_ENDPOINT_ID\n",
    "        , ME_INDEX_ID\n",
    "        , me_index\n",
    "        , index_endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573a659-3967-4012-9406-32880027cc81",
   "metadata": {},
   "source": [
    "### Component to Deploy Index to Index Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f2017f4-d849-4196-bda2-abada4581ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/zghost-{ACTOR_PREFIX}/gdelt-pipe-{VERSION}:latest\"\n",
    ")\n",
    "def deploy_index(\n",
    "    project: str\n",
    "    , project_num: str\n",
    "    , location: str\n",
    "    , version: str\n",
    "    , me_index_id: str\n",
    "    , me_index_endpoint_id: str\n",
    "    , me_index_name: str\n",
    "    , me_index_endpoint_name: str\n",
    "    , vpc_network_full: str\n",
    "    \n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('index_endpoint_deployed', Artifact)\n",
    "    , ('me_index_endpoint_id', str)\n",
    "    , ('me_index_name', str)\n",
    "]):\n",
    "    import logging\n",
    "    # ======================================\n",
    "    # Import packages\n",
    "    # ======================================\n",
    "    logging.info(f\"Importing packages...\")\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    \n",
    "    from zeitghost.vertex.MatchingEngineCRUD import MatchingEngineCRUD\n",
    "\n",
    "    # log component args\n",
    "    logging.info(f\"me_index_id                : {me_index_id}\")\n",
    "    logging.info(f\"me_index_endpoint_id       : {me_index_endpoint_id}\")\n",
    "    logging.info(f\"me_index_name              : {me_index_name}\")\n",
    "    logging.info(f\"me_index_endpoint_name     : {me_index_endpoint_name}\")\n",
    "    logging.info(f\"vpc_network_full           : {vpc_network_full}\")\n",
    "    \n",
    "    # ======================================\n",
    "    # create mengine\n",
    "    # ======================================\n",
    "    logging.info(f\"create mengine...\")\n",
    "    \n",
    "    mengine = MatchingEngineCRUD(\n",
    "        project_id=project \n",
    "        , project_num=project_num\n",
    "        , region=location \n",
    "        , index_name=me_index_name\n",
    "        , vpc_network_name=vpc_network_full\n",
    "    )\n",
    "    \n",
    "    # ======================================\n",
    "    # deploy ME index \n",
    "    # ======================================\n",
    "    logging.info(f\"deploying index...\")\n",
    "    index_endpoint = mengine.deploy_index(\n",
    "        index_name = me_index_name\n",
    "        , endpoint_name = me_index_endpoint_name\n",
    "    )\n",
    "    \n",
    "    if index_endpoint:\n",
    "        logging.info(f\"Index endpoint resource name: {index_endpoint.name}\")\n",
    "        logging.info(f\"Index endpoint VPC network name: {index_endpoint.network}\")\n",
    "        logging.info(f\"Deployed indexes on the index endpoint:\")\n",
    "        for d in index_endpoint.deployed_indexes:\n",
    "            logging.info(f\"    {d.id}\")\n",
    "            \n",
    "    return (\n",
    "        index_endpoint\n",
    "        , me_index_endpoint_id\n",
    "        , me_index_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd87b1-4a4a-4e6f-802e-f1a1e99929b0",
   "metadata": {},
   "source": [
    "### Component to Load GDELT Records to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e8b711c-5a33-4b1a-957d-9891defd7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/zghost-{ACTOR_PREFIX}/gdelt-pipe-{VERSION}:latest\"\n",
    ")\n",
    "def load_gdelt_events(\n",
    "    project: str\n",
    "    , project_num: str\n",
    "    , location: str\n",
    "    , version: str\n",
    "    , bq_dataset: str\n",
    "    , vpc_network_full: str\n",
    "    , embedding_dir_bucket_name: str\n",
    "    , embedding_dir_bucket_uri: str\n",
    "    , table_name: str\n",
    "    , me_index_endpoint_id: str\n",
    "    , me_index_id: str\n",
    "    , me_index_artifact: Input[Artifact]\n",
    "    , me_index_endpoint_artifact: Input[Artifact]\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('me_index_endpoint_id', str)\n",
    "    , ('me_index_id', str)\n",
    "    , ('uploaded_ids_list', list)\n",
    "    , ('uploaded_ids', Artifact)\n",
    "]):\n",
    "        \n",
    "    import logging\n",
    "    # ====================================\n",
    "    # import packages\n",
    "    # ====================================\n",
    "    logging.info(f\"importing packages...\")\n",
    "    \n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    import uuid\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import time\n",
    "    import io\n",
    "    import os\n",
    "    \n",
    "    from langchain.document_loaders import DataFrameLoader\n",
    "    from langchain.docstore.document import Document\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery as bq\n",
    "    \n",
    "    from zeitghost.vertex.MatchingEngineVectorstore import MatchingEngineVectorStore\n",
    "    from zeitghost.vertex.LLM import VertexLLM\n",
    "    from zeitghost.vertex.Embeddings import VertexEmbeddings\n",
    "    \n",
    "    # set project ID\n",
    "    CLOUD_ML_PROJECT_ID = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    \n",
    "    # log component args\n",
    "    logging.info(f\"bq_dataset: {bq_dataset}\")\n",
    "    logging.info(f\"vpc_network_full: {vpc_network_full}\")\n",
    "    logging.info(f\"table_name: {table_name}\")\n",
    "    logging.info(f\"CLOUD_ML_PROJECT_ID: {CLOUD_ML_PROJECT_ID}\")\n",
    "    \n",
    "    # GCP SDK clients\n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    # bigquery client\n",
    "    bqclient = bq.Client(\n",
    "        project=CLOUD_ML_PROJECT_ID,\n",
    "        # location=LOCATION\n",
    "    )\n",
    "    # helper function\n",
    "    def test_gcs_blob_metadata(blob_name, bucket_name):\n",
    "        \"\"\"\n",
    "        inspect blobs uploaded to GCS\n",
    "        \"\"\"\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.get_blob(blob_name)\n",
    "        print(f\"Metadata: {blob.metadata}\")\n",
    "        \n",
    "    logging.info(f\"table_ref: {table_name}\")\n",
    "    \n",
    "    # ======================================\n",
    "    # create Vertex LLM\n",
    "    # ======================================\n",
    "    logging.info(f\"create ME vector store...\")\n",
    "    \n",
    "    llm = VertexLLM(\n",
    "        stop=None \n",
    "        , temperature=0.0\n",
    "        , max_output_tokens=1000\n",
    "        , top_p=0.7\n",
    "        , top_k=40\n",
    "    )\n",
    "\n",
    "    REQUESTS_PER_MINUTE = 299 # project quota==300\n",
    "    logging.info(f\"REQUESTS_PER_MINUTE: {REQUESTS_PER_MINUTE}\")\n",
    "    \n",
    "    vertex_embedding = VertexEmbeddings(requests_per_minute=REQUESTS_PER_MINUTE)\n",
    "    \n",
    "    # ======================================\n",
    "    # create ME vector store\n",
    "    # ======================================\n",
    "    logging.info(f\"create ME vector store...\")\n",
    "    me = MatchingEngineVectorStore.from_components(\n",
    "        project_id=project\n",
    "        # , project_num=PROJECT_NUM\n",
    "        , region=location\n",
    "        , gcs_bucket_name=embedding_dir_bucket_uri\n",
    "        , embedding=vertex_embedding\n",
    "        , index_id=me_index_id\n",
    "        , endpoint_id=me_index_endpoint_id\n",
    "    )\n",
    "    \n",
    "    # ======================================\n",
    "    # load gdelt table\n",
    "    # ======================================\n",
    "    logging.info(f\"loading gdelt table...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT * \n",
    "        FROM `{table_name}`\n",
    "        -- LIMIT 5\n",
    "    \"\"\"\n",
    "    logging.info(f\"query: {query}\")\n",
    "    df=bqclient.query(query).to_dataframe().head(1)\n",
    "\n",
    "    df_col_list = df.columns\n",
    "    logging.info(f\"df_col_list: {df_col_list}\")\n",
    "    \n",
    "    meta_cols_list = list(df_col_list)\n",
    "    meta_cols_list.remove('text')\n",
    "    logging.info(f\"meta_cols_list: {meta_cols_list}\")\n",
    "    \n",
    "    # ======================================\n",
    "    # chunk text \n",
    "    # ======================================\n",
    "    logging.info(f\"chunk text...\")\n",
    "    start = time.time()\n",
    "\n",
    "    docs = me.chunk_bq_table(\n",
    "        bq_dataset_name=bq_dataset\n",
    "        , bq_table_name=table_name\n",
    "        , query=query\n",
    "        , page_content_cols=['text']\n",
    "        # , metadata_cols=['source']\n",
    "        , metadata_cols=meta_cols_list\n",
    "        , chunk_size=1000\n",
    "        , chunk_overlap=0\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    logging.info(f\"elapsed time: {end - start}\")\n",
    "\n",
    "    texts = [d.page_content for d in docs]\n",
    "    metas = [d.metadata for d in docs]\n",
    "\n",
    "    # chunk text and add to matching engine vector store\n",
    "    uploaded_ids = me.add_texts(\n",
    "        texts=texts\n",
    "        , metadatas=metas\n",
    "    )\n",
    "    logging.info(f\"uploaded_ids: {uploaded_ids}\")\n",
    "    \n",
    "    uuid_strings = []\n",
    "\n",
    "    for uid in uploaded_ids:\n",
    "        uuid_strings.append(str(uid))\n",
    "        \n",
    "    # ======================================\n",
    "    # test gcs blob metadata\n",
    "    # ======================================\n",
    "    logging.info(f\"testing gcs blob metadata...\")\n",
    "        \n",
    "    # test blob metadata\n",
    "    TEST_BLOB_UUID = str(uuid_strings[0])\n",
    "    logging.info(TEST_BLOB_UUID)\n",
    "\n",
    "    BLOB_NAME=f'documents/{TEST_BLOB_UUID}'\n",
    "    logging.info(\n",
    "        test_gcs_blob_metadata(\n",
    "            blob_name=BLOB_NAME\n",
    "            , bucket_name=embedding_dir_bucket_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        me_index_endpoint_id\n",
    "        , me_index_id\n",
    "        , uuid_strings\n",
    "        , uuid_strings\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9cb73-7388-490d-a9ca-de0bcc1c506b",
   "metadata": {},
   "source": [
    "### Component to Load GDELT GEG (Entity) articles to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fce3686e-1d53-4c8c-af57-1a80ec1b7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "    base_image=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/zghost-{ACTOR_PREFIX}/gdelt-pipe-{VERSION}:latest\"\n",
    ")\n",
    "def load_gdelt_geg_articles(\n",
    "    project: str\n",
    "    , project_num: str\n",
    "    , location: str\n",
    "    , version: str\n",
    "    , bq_dataset: str\n",
    "    , vpc_network_full: str\n",
    "    , embedding_dir_bucket_name: str\n",
    "    , embedding_dir_bucket_uri: str\n",
    "    , table_name: str\n",
    "    , me_index_endpoint_id: str\n",
    "    , me_index_id: str\n",
    "    , me_index_artifact: Input[Artifact]\n",
    "    , me_index_endpoint_artifact: Input[Artifact]\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('me_index_endpoint_id', str)\n",
    "    , ('me_index_id', str)\n",
    "    , ('uploaded_ids_list', list)\n",
    "    , ('uploaded_ids', Artifact)\n",
    "]):\n",
    "    \n",
    "    import logging\n",
    "    # ====================================\n",
    "    # import packages\n",
    "    # ====================================\n",
    "    logging.info(f\"importing packages...\")\n",
    "    \n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    import uuid\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import time\n",
    "    import io\n",
    "    import os\n",
    "    \n",
    "    from langchain.document_loaders import DataFrameLoader\n",
    "    from langchain.docstore.document import Document\n",
    "    \n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import bigquery as bq\n",
    "    \n",
    "    from zeitghost.vertex.MatchingEngineVectorstore import MatchingEngineVectorStore\n",
    "    from zeitghost.vertex.LLM import VertexLLM\n",
    "    from zeitghost.vertex.Embeddings import VertexEmbeddings\n",
    "    \n",
    "    # set project ID\n",
    "    CLOUD_ML_PROJECT_ID = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "    \n",
    "    # log component args\n",
    "    logging.info(f\"bq_dataset: {bq_dataset}\")\n",
    "    logging.info(f\"vpc_network_full: {vpc_network_full}\")\n",
    "    logging.info(f\"table_name: {table_name}\")\n",
    "    logging.info(f\"CLOUD_ML_PROJECT_ID: {CLOUD_ML_PROJECT_ID}\")\n",
    "    \n",
    "    # GCP SDK clients\n",
    "    storage_client = storage.Client(project=project)\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    # bigquery client\n",
    "    bqclient = bq.Client(\n",
    "        project=CLOUD_ML_PROJECT_ID,\n",
    "        # location=LOCATION\n",
    "    )\n",
    "    # helper function\n",
    "    def test_gcs_blob_metadata(blob_name, bucket_name):\n",
    "        \"\"\"\n",
    "        inspect blobs uploaded to GCS\n",
    "        \"\"\"\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.get_blob(blob_name)\n",
    "        print(f\"Metadata: {blob.metadata}\")\n",
    "\n",
    "    logging.info(f\"table_name: {table_name}\")\n",
    "    \n",
    "    # ======================================\n",
    "    # create Vertex LLM\n",
    "    # ======================================\n",
    "    logging.info(f\"create ME vector store...\")\n",
    "    \n",
    "    llm = VertexLLM(\n",
    "        stop=None \n",
    "        , temperature=0.0\n",
    "        , max_output_tokens=1000\n",
    "        , top_p=0.7\n",
    "        , top_k=40\n",
    "    )\n",
    "\n",
    "    REQUESTS_PER_MINUTE = 299 # project quota==300\n",
    "    logging.info(f\"REQUESTS_PER_MINUTE: {REQUESTS_PER_MINUTE}\")\n",
    "    \n",
    "    vertex_embedding = VertexEmbeddings(requests_per_minute=REQUESTS_PER_MINUTE)\n",
    "    \n",
    "    # ======================================\n",
    "    # create ME vector store\n",
    "    # ======================================\n",
    "    logging.info(f\"create ME vector store...\")\n",
    "    me = MatchingEngineVectorStore.from_components(\n",
    "        project_id=project\n",
    "        # , project_num=PROJECT_NUM\n",
    "        , region=location\n",
    "        , gcs_bucket_name=embedding_dir_bucket_uri\n",
    "        , embedding=vertex_embedding\n",
    "        , index_id=me_index_id\n",
    "        , endpoint_id=me_index_endpoint_id\n",
    "    )\n",
    "    \n",
    "    # ======================================\n",
    "    # load gdelt table\n",
    "    # ======================================\n",
    "    logging.info(f\"loading gdelt table...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT * \n",
    "        FROM `{table_name}`\n",
    "        -- LIMIT 5\n",
    "    \"\"\"\n",
    "    logging.info(f\"query: {query}\")\n",
    "    df=bqclient.query(query).to_dataframe().head(1)\n",
    "\n",
    "    df_col_list = df.columns\n",
    "    logging.info(f\"df_col_list: {df_col_list}\")\n",
    "    \n",
    "    meta_cols_list = list(df_col_list)\n",
    "    meta_cols_list = [\n",
    "        e for e in meta_cols_list if e not in (\n",
    "            'text'\n",
    "            , 'language'\n",
    "            , 'date'\n",
    "            , 'Actor1Name'\n",
    "            , 'Actor2Name'\n",
    "            , 'GoldsteinScale'\n",
    "        )\n",
    "    ]\n",
    "    logging.info(f\"meta_cols_list: {meta_cols_list}\")\n",
    "    \n",
    "    # ======================================\n",
    "    # chunk text \n",
    "    # ======================================\n",
    "    logging.info(f\"chunk text...\")\n",
    "    start = time.time()\n",
    "\n",
    "    docs = me.chunk_bq_table(\n",
    "        bq_dataset_name=bq_dataset\n",
    "        , bq_table_name=table_name\n",
    "        , query=query\n",
    "        , page_content_cols=['text']\n",
    "        # , metadata_cols=['source']\n",
    "        , metadata_cols=meta_cols_list\n",
    "        , chunk_size=1000\n",
    "        , chunk_overlap=0\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    logging.info(f\"elapsed time: {end - start}\")\n",
    "\n",
    "    texts = [d.page_content for d in docs]\n",
    "    metas = [d.metadata for d in docs]\n",
    "\n",
    "    # chunk text and add to matching engine vector store\n",
    "    uploaded_ids = me.add_texts(\n",
    "        texts=texts\n",
    "        , metadatas=metas\n",
    "    )\n",
    "    logging.info(f\"uploaded_ids: {uploaded_ids}\")\n",
    "    \n",
    "    uuid_strings = []\n",
    "\n",
    "    for uid in uploaded_ids:\n",
    "        uuid_strings.append(str(uid))\n",
    "        \n",
    "    # ======================================\n",
    "    # test gcs blob metadata\n",
    "    # ======================================\n",
    "    logging.info(f\"testing gcs blob metadata...\")\n",
    "    \n",
    "    TEST_BLOB_UUID = str(uuid_strings[0])\n",
    "    logging.info(TEST_BLOB_UUID)\n",
    "\n",
    "    BLOB_NAME=f'documents/{TEST_BLOB_UUID}'\n",
    "    logging.info(\n",
    "        test_gcs_blob_metadata(\n",
    "            blob_name=BLOB_NAME\n",
    "            , bucket_name=embedding_dir_bucket_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        me_index_endpoint_id\n",
    "        , me_index_id\n",
    "        , uuid_strings\n",
    "        , uuid_strings\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc88ff-af12-4c96-802c-d5461d86de1d",
   "metadata": {},
   "source": [
    "## Pipeline Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82035016-fe99-42a9-9712-b987b13a4515",
   "metadata": {},
   "source": [
    "### Set Pipeline Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "32cfa78a-6561-46d4-a759-bb738afbdf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE_NEW_ASSETS  : False\n",
      "ACTOR_PREFIX       : way\n",
      "VERSION            : v1\n"
     ]
    }
   ],
   "source": [
    "# create new assets in pipeline\n",
    "CREATE_NEW_ASSETS        = \"False\" # TODO: \"True\" | \"False\"\n",
    "# ACTOR_PREFIX             = \"way\"  # TODO\n",
    "# VERSION                  = \"v1\"   # TODO\n",
    "\n",
    "print(f\"CREATE_NEW_ASSETS  : {CREATE_NEW_ASSETS}\")\n",
    "print(f\"ACTOR_PREFIX       : {ACTOR_PREFIX}\")\n",
    "print(f\"VERSION            : {VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "16f8629e-adb4-4e8f-b53b-b43288c9c4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_URI: gs://zghost-way-v1-wortz-project-352116\n",
      "EMBEDDING_DIR_BUCKET_URI: gs://zghost-way-v1-wortz-project-352116-emd-dir\n",
      "PIPELINE_ROOT_PATH: gs://zghost-way-v1-wortz-project-352116/pipeline_root/v1\n"
     ]
    }
   ],
   "source": [
    "# change pipeline_version as you make tweaks to the pipeline \n",
    "PIPELINE_VERSION='v1'\n",
    "\n",
    "# Stores pipeline executions for each run\n",
    "PIPELINE_ROOT_PATH = f'{BUCKET_URI}/pipeline_root/{PIPELINE_VERSION}'\n",
    "\n",
    "print(f\"BUCKET_URI: {BUCKET_URI}\")\n",
    "print(f'EMBEDDING_DIR_BUCKET_URI: {EMBEDDING_DIR_BUCKET_URI}')\n",
    "print(f'PIPELINE_ROOT_PATH: {PIPELINE_ROOT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8e5a7938-0a0e-455d-9ac7-7f923fb82df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_NAME: gdelt-pipe-v1-way-v1\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME = f'gdelt-pipe-{PIPELINE_VERSION}-{ACTOR_PREFIX}-{VERSION}'.replace('_', '-')\n",
    "print(f\"PIPELINE_NAME: {PIPELINE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a894fc",
   "metadata": {},
   "source": [
    "Create pipeline definition from the previously defined components\n",
    "\n",
    "Adds conditional logic to only deploy new infrastructure if it does not previously exist - see flag `CREATE_NEW_ASSETS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d9b3aa41-94bf-4af8-8c72-0f97d052c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.pipeline(\n",
    "  name=PIPELINE_NAME\n",
    ")\n",
    "def pipeline(\n",
    "    project: str\n",
    "    , project_num: str\n",
    "    , location: str\n",
    "    , version: str\n",
    "    , bq_dataset: str\n",
    "    , min_date_events: str\n",
    "    , max_date_events: str\n",
    "    , min_date_geg: str\n",
    "    , max_date_geg: str\n",
    "    , test_table_prefix: str\n",
    "    , vpc_network_full: str\n",
    "    , extract_geg_articles: str\n",
    "    , extract_gdelt_events: str\n",
    "    , create_new_assets: str\n",
    "    , actor_prefix: str\n",
    "    , actor_name: str\n",
    "    , vpc_network_name: str\n",
    "    , bq_location: str = 'US'\n",
    "    , create_me_endpoint: str = 'False'\n",
    "):\n",
    "    \n",
    "    \n",
    "    # ======================================\n",
    "    # create new or get existing ME index\n",
    "    # ======================================\n",
    "    set_config_op = (\n",
    "        set_config(\n",
    "            project=project\n",
    "            , project_num=project_num\n",
    "            , location=location\n",
    "            , version=version\n",
    "            , actor_prefix=actor_prefix\n",
    "            , actor_name=actor_name\n",
    "            , vpc_network_name=vpc_network_name\n",
    "            , create_new_assets=create_new_assets\n",
    "        )\n",
    "        .set_caching_options(True)\n",
    "        .set_display_name(\"pipeline config\")\n",
    "    )\n",
    "    \n",
    "    # ======================================\n",
    "    # create new or get existing ME index\n",
    "    # ======================================\n",
    "    create_me_vectorstore_op = (\n",
    "        create_me_vectorstore(\n",
    "            project=project\n",
    "            , project_num=project_num\n",
    "            , location=location\n",
    "            , version=version\n",
    "            , bq_dataset=set_config_op.outputs['my_bq_dataset']\n",
    "            , me_index_name=set_config_op.outputs['me_index_name']\n",
    "            , me_index_endpoint_name=set_config_op.outputs['me_index_endpoint_name']\n",
    "            , vpc_network_full=set_config_op.outputs['vpc_network_full']\n",
    "            , embedding_dir_bucket_uri=set_config_op.outputs['emb_bucket_uri']\n",
    "            , me_dimensions=set_config_op.outputs['me_dimensions']\n",
    "        )\n",
    "        .set_caching_options(True)\n",
    "        .set_display_name(\"create or get ME index & endpoint\")\n",
    "        # .set_cpu_limit(XXXX)\n",
    "        # .set_memory_limit(XXXX)\n",
    "    )\n",
    "    \n",
    "    # ======================================\n",
    "    # deploy index\n",
    "    # ======================================\n",
    "    with kfp.v2.dsl.Condition(create_me_endpoint == \"True\", name=\"deploy_index\"):\n",
    "        \n",
    "        deploy_index_op = (\n",
    "            deploy_index(\n",
    "                project=project\n",
    "                , project_num=project_num\n",
    "                , location=location\n",
    "                , version=version\n",
    "                , me_index_id=create_me_vectorstore_op.outputs['me_index_id']\n",
    "                , me_index_endpoint_id=create_me_vectorstore_op.outputs['me_index_endpoint_id']\n",
    "                , me_index_endpoint_name=set_config_op.outputs['me_index_endpoint_name']\n",
    "                , me_index_name=set_config_op.outputs['me_index_name']\n",
    "                , vpc_network_full=set_config_op.outputs['vpc_network_full']\n",
    "            )\n",
    "            .set_display_name(\"deploy index to endpoint\")\n",
    "        )\n",
    "    \n",
    "    with kfp.v2.dsl.Condition(extract_gdelt_events == \"True\", name=\"extract event articles\"):\n",
    "        \n",
    "        # ======================================\n",
    "        # get GDELT events articles\n",
    "        # ======================================\n",
    "        create_gdelt_events_table_op = (\n",
    "            create_gdelt_events_table(\n",
    "                project=project\n",
    "                , location=location\n",
    "                , bq_location=bq_location\n",
    "                , version=version\n",
    "                , bq_dataset=set_config_op.outputs['my_bq_dataset']\n",
    "                , actor_prefix=actor_prefix\n",
    "                , actor_name=actor_name\n",
    "                , min_date=min_date_events\n",
    "                , max_date=max_date_events\n",
    "                , test_table_prefix=test_table_prefix\n",
    "            )\n",
    "            .set_caching_options(True)\n",
    "            .set_display_name(\"Scrape gdelt event articles\")\n",
    "            # .set_cpu_limit(XXXX)\n",
    "            # .set_memory_limit(XXXX)\n",
    "        )\n",
    "\n",
    "        with kfp.v2.dsl.Condition(create_gdelt_events_table_op.outputs['scraped_articles_table'] != \"\", name=\"new event articles\"):\n",
    "\n",
    "            # ======================================\n",
    "            # index GDELT events in ME index\n",
    "            # ======================================\n",
    "            load_gdelt_events_op = (\n",
    "                load_gdelt_events(\n",
    "                    project=project\n",
    "                    , project_num=project_num\n",
    "                    , location=location\n",
    "                    , version=version\n",
    "                    , bq_dataset=set_config_op.outputs['my_bq_dataset']\n",
    "                    , vpc_network_full=set_config_op.outputs['vpc_network_full']\n",
    "                    , embedding_dir_bucket_name=set_config_op.outputs['emb_bucket']\n",
    "                    , embedding_dir_bucket_uri=set_config_op.outputs['emb_bucket_uri']\n",
    "                    , table_name=create_gdelt_events_table_op.outputs['scraped_articles_table']\n",
    "                    , me_index_endpoint_id=create_me_vectorstore_op.outputs['me_index_endpoint_id']\n",
    "                    , me_index_id=create_me_vectorstore_op.outputs['me_index_id']\n",
    "                    , me_index_artifact=create_me_vectorstore_op.outputs['me_index']\n",
    "                    , me_index_endpoint_artifact=create_me_vectorstore_op.outputs['index_endpoint']\n",
    "                )\n",
    "                .set_caching_options(True)\n",
    "                .set_display_name(\"Index gdelt event articles in ME\")\n",
    "                # .set_cpu_limit(XXXX)\n",
    "                # .set_memory_limit(XXXX)\n",
    "            )\n",
    "\n",
    "    with kfp.v2.dsl.Condition(extract_geg_articles == \"True\", name=\"extract geg articles\"):\n",
    "        \n",
    "        # ======================================\n",
    "        # get GDELT geg articles\n",
    "        # ======================================\n",
    "        create_geg_article_table_op = (\n",
    "            create_geg_article_table(\n",
    "                project=project\n",
    "                , location=location\n",
    "                , bq_location=bq_location\n",
    "                , version=version\n",
    "                , bq_dataset=set_config_op.outputs['my_bq_dataset']\n",
    "                , actor_prefix=actor_prefix\n",
    "                , actor_name=actor_name\n",
    "                , min_date=min_date_geg\n",
    "                , max_date=max_date_geg\n",
    "                , test_table_prefix=test_table_prefix\n",
    "            )\n",
    "            .set_caching_options(True)\n",
    "            .set_display_name(\"Scrape gdelt geg articles\")\n",
    "            # .set_cpu_limit(XXXX)\n",
    "            # .set_memory_limit(XXXX)\n",
    "        )\n",
    "        \n",
    "        with kfp.v2.dsl.Condition(create_geg_article_table_op.outputs['scraped_articles_table'] != \"\", name=\"new geg articles\"):\n",
    "            # ======================================\n",
    "            # index GDELT geg in ME index\n",
    "            # ======================================\n",
    "            load_gdelt_geg_articles_op = (\n",
    "                load_gdelt_geg_articles(\n",
    "                    project=project\n",
    "                    , project_num=project_num\n",
    "                    , location=location\n",
    "                    , version=version\n",
    "                    , bq_dataset=set_config_op.outputs['my_bq_dataset']\n",
    "                    , vpc_network_full=set_config_op.outputs['vpc_network_full']\n",
    "                    , embedding_dir_bucket_name=set_config_op.outputs['emb_bucket']\n",
    "                    , embedding_dir_bucket_uri=set_config_op.outputs['emb_bucket_uri']\n",
    "                    , table_name=create_geg_article_table_op.outputs['scraped_articles_table']\n",
    "                    , me_index_endpoint_id=create_me_vectorstore_op.outputs['me_index_endpoint_id']\n",
    "                    , me_index_id=create_me_vectorstore_op.outputs['me_index_id']\n",
    "                    , me_index_artifact=create_me_vectorstore_op.outputs['me_index']\n",
    "                    , me_index_endpoint_artifact=create_me_vectorstore_op.outputs['index_endpoint']\n",
    "                )\n",
    "                .set_caching_options(True)\n",
    "                .set_display_name(\"Index gdelt geg articles in ME\")\n",
    "                # .set_cpu_limit(XXXX)\n",
    "                # .set_memory_limit(XXXX)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cfe8a-d5cb-4ab1-b9a8-ea1f80bdf8ae",
   "metadata": {},
   "source": [
    "### Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fe9de7fd-7569-45e0-8a05-002409f4003c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.9/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_JSON_SPEC_LOCAL = \"custom_pipeline_spec.json\"\n",
    "\n",
    "! rm -f ./pipelines/$PIPELINE_JSON_SPEC_LOCAL\n",
    "\n",
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=PIPELINE_JSON_SPEC_LOCAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3f15969f-59a3-49d5-aad5-b6e968dc4902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINES_FILEPATH: gs://zghost-way-v1-wortz-project-352116/pipeline_root/v1/pipeline_spec.json\n",
      "Copying file://custom_pipeline_spec.json [Content-Type=application/json]...\n",
      "/ [1 files][ 94.8 KiB/ 94.8 KiB]                                                \n",
      "Operation completed over 1 objects/94.8 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "PIPELINES_FILEPATH = f'{PIPELINE_ROOT_PATH}/pipeline_spec.json'\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)\n",
    "\n",
    "!gsutil cp $PIPELINE_JSON_SPEC_LOCAL $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5c833993-910f-41fa-8c51-3b1bc28175e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://zghost-way-v1-wortz-project-352116/pipeline_root/v1/pipeline_spec.json\n",
      "gs://zghost-way-v1-wortz-project-352116/pipeline_root/v1/679926387543/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $PIPELINE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490bacd-a987-4504-953b-0e9fbaaa72b0",
   "metadata": {},
   "source": [
    "### Submit Pipeline Job\n",
    "All Vertex AI Pipelines use service accounts - make sure that the service account that you are using has the required permissions to perform the activities in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9841e47d-6ed6-4f69-905d-e2c2b68a57e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_TABLE_PREFIX     : True\n",
      "MIN_DATE_EVENTS       : 2023-06-01\n",
      "MAX_DATE_EVENTS       : 2023-06-14\n",
      "MIN_DATE_GEG          : 2023-01-01\n",
      "MAX_DATE_GEG          : 2023-06-14\n",
      "EXTRACT_GDELT_EVENTS  : True\n",
      "EXTRACT_GDELT_GEG     : True\n"
     ]
    }
   ],
   "source": [
    "# Setting this to true to publish the endpoint in case needed - default is False\n",
    "CREATE_ME_ENDPOINT = \"False\"\n",
    "\n",
    "# \"True\" will write results to a table prefixed with `test`; useful for debugging and testing pipeline\n",
    "# change to \"False\" when scraping all GDELT records of interest\n",
    "TEST_TABLE_PREFIX    = \"True\" # TODO: \"True\" | \"False\"\n",
    "\n",
    "# set time window for GDELT `events` table\n",
    "MIN_DATE_EVENTS      = \"2023-06-01\"  # TODO\n",
    "MAX_DATE_EVENTS      = \"2023-06-14\"  # TODO\n",
    "\n",
    "# set time window for GDELT `global entity graph` (geg) table\n",
    "MIN_DATE_GEG         = \"2023-01-01\"  # TODO\n",
    "MAX_DATE_GEG         = \"2023-06-14\"  # TODO\n",
    "\n",
    "# conditional pipeline args; usefull when testing and debugging\n",
    "EXTRACT_GDELT_EVENTS = \"True\"        # TODO: 'True' | 'False'\n",
    "EXTRACT_GDELT_GEG    = \"True\"        # TODO: 'True' | 'False'\n",
    "\n",
    "print(f\"TEST_TABLE_PREFIX     : {TEST_TABLE_PREFIX}\")\n",
    "print(f\"MIN_DATE_EVENTS       : {MIN_DATE_EVENTS}\")\n",
    "print(f\"MAX_DATE_EVENTS       : {MAX_DATE_EVENTS}\")\n",
    "print(f\"MIN_DATE_GEG          : {MIN_DATE_GEG}\")\n",
    "print(f\"MAX_DATE_GEG          : {MAX_DATE_GEG}\")\n",
    "print(f\"EXTRACT_GDELT_EVENTS  : {EXTRACT_GDELT_EVENTS}\")\n",
    "print(f\"EXTRACT_GDELT_GEG     : {EXTRACT_GDELT_GEG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3d59c4b0-ffb8-47f1-993a-d3bad783dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VPC_NETWORK_FULL = f'projects/{PROJECT_NUM}/global/networks/{vpc_network_name}'\n",
    "SERVICE_ACCOUNT = f'{PROJECT_NUM}-compute@developer.gserviceaccount.com'\n",
    "\n",
    "job = vertex_ai.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINES_FILEPATH,\n",
    "    pipeline_root=f'{PIPELINE_ROOT_PATH}',\n",
    "    failure_policy='fast', # slow | fast\n",
    "    # enable_caching=False,\n",
    "    parameter_values={\n",
    "        'project'                    : PROJECT_ID\n",
    "        , 'project_num'              : PROJECT_NUM\n",
    "        , 'location'                 : LOCATION\n",
    "        , 'bq_location'              : BQ_LOCATION\n",
    "        , 'version'                  : VERSION\n",
    "        , 'vpc_network_full'         : VPC_NETWORK_FULL\n",
    "        , 'bq_dataset'               : MY_BQ_DATASET\n",
    "        , 'min_date_events'          : MIN_DATE_EVENTS\n",
    "        , 'max_date_events'          : MAX_DATE_EVENTS\n",
    "        , 'min_date_geg'             : MIN_DATE_GEG\n",
    "        , 'max_date_geg'             : MAX_DATE_GEG\n",
    "        , 'test_table_prefix'        : TEST_TABLE_PREFIX\n",
    "        , 'extract_gdelt_events'     : EXTRACT_GDELT_EVENTS\n",
    "        , 'extract_geg_articles'     : EXTRACT_GDELT_GEG\n",
    "        , 'create_new_assets'        : CREATE_NEW_ASSETS\n",
    "        , 'actor_name'               : ACTOR_NAME\n",
    "        , 'actor_prefix'             : ACTOR_PREFIX\n",
    "        , 'vpc_network_name'         : VPC_NETWORK_NAME\n",
    "        , 'create_me_endpoint'       : CREATE_ME_ENDPOINT\n",
    "    },\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=False,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    network=VPC_NETWORK_FULL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225388a",
   "metadata": {},
   "source": [
    "In the console, if you navigate within Vertex AI to pipeline runs, you should see your pipeline running after you submit the job"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
